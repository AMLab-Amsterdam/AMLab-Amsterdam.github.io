<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://amlab-amsterdam.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://amlab-amsterdam.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-03-06T16:17:53+00:00</updated><id>https://amlab-amsterdam.github.io/feed.xml</id><title type="html">blank</title><subtitle>Amsterdam Machine Learning Lab, Informatics Institute, University of Amsterdam 
</subtitle><entry><title type="html">AMLab Papers at NeurIPS 2023</title><link href="https://amlab-amsterdam.github.io/blog/2023/neurips/" rel="alternate" type="text/html" title="AMLab Papers at NeurIPS 2023" /><published>2023-11-20T07:00:00+00:00</published><updated>2023-11-20T07:00:00+00:00</updated><id>https://amlab-amsterdam.github.io/blog/2023/neurips</id><content type="html" xml:base="https://amlab-amsterdam.github.io/blog/2023/neurips/"><![CDATA[<div style="background-color: rgba(123, 104, 238, 0.1); padding: 10px 0; text-align: center;">
    <img src="https://neurips.cc/media/Press/NeurIPS_logo.svg" alt="NeurIPS" width="240" height="120" style="margin: auto;" />
</div>

<hr style="margin-top: 20px;" />

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Clifford Group Equivariant Neural Networks</strong><br />
      <em>David Ruhe, Johannes Brandstetter, Patrick Forré</em><br />

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://openreview.net/forum?id=n84bzMrGUD&amp;referrer=%5BAuthor%20Console%5D(%2Fgroup%5DNeurIPS.cc%2F2023%2FConference%2FAuthors%23your-submissions)" style="text-decoration: none; color: #007bff;">Conference paper</a> &middot; 
        <a href="https://openreview.net/pdf?id=n84bzMrGUD" style="text-decoration: none; color: #007bff;">PDF</a> &middot; 
        <a href="https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks" style="text-decoration: none; color: #007bff;">GitHub repo</a>
      </span>

      <strong>Abstract:</strong> We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing O(n)- and E(n)-equivariant models. We identify and study the Clifford group: a subgroup inside the Clifford algebra tailored to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, including their grade projections, constitutes an equivariant map with respect to the Clifford group, allowing us to parameterize equivariant neural network layers. An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a single core implementation, state-of-the-art performance on several distinct tasks, including a three-dimensional N-body experiment, a four-dimensional Lorentz-equivariant high-energy physics experiment, and a five-dimensional convex hull experiment. <br />

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #GeometricAlgebra #Equivariance #RepresentationTheory
      </span>

    </td>
    <td style="position: relative; width: 50%;">
      <img src="/assets/neurips2023/clifford.png" alt="Graphical Abstract" style="height: auto; max-height: 90%; width: auto; max-width: 90%;  margin-left: 5%;" />
      <span style="position: absolute; top: 14%; right: 0; background-color: #ffcc00; color: black; padding: 5px; font-size: 20px; transform: rotate(35deg); transform-origin: top right;">
        Oral Presentation
      </span>
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;" />

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Rotating Features for Object Discovery</strong><br />
      <em>Sindy Löwe, Phillip Lippe, Francesco Locatello, Max Welling</em><br />

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://arxiv.org/abs/2306.00600" style="text-decoration: none; color: #007bff;">Paper</a> &middot; 
        <a href="https://arxiv.org/pdf/2306.00600.pdf" style="text-decoration: none; color: #007bff;">PDF</a> &middot; 
        <a href="https://github.com/loeweX/RotatingFeatures" style="text-decoration: none; color: #007bff;">GitHub repo</a>
      </span>

      <strong>Abstract:</strong> The binding problem in human cognition, concerning how the brain represents and connects objects within a fixed network of neural connections, remains a subject of intense debate. Most machine learning efforts addressing this issue in an unsupervised setting have focused on slot-based methods, which may be limiting due to their discrete nature and difficulty to express uncertainty. Recently, the Complex AutoEncoder was proposed as an alternative that learns continuous and distributed object-centric representations. However, it is only applicable to simple toy data. In this paper, we present Rotating Features, a generalization of complex-valued features to higher dimensions, and a new evaluation procedure for extracting objects from distributed representations. Additionally, we show the applicability of our approach to pre-trained features. Together, these advancements enable us to scale distributed object-centric representations from simple toy to real-world data. We believe this work advances a new paradigm for addressing the binding problem in machine learning and has the potential to inspire further innovation in the field. <br />

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #ObjectDiscovery #RepresentationLearning #ComplexAutoEncoder
      </span>

    </td>
    <td style="position: relative; width: 50%;">
      <img src="https://github.com/loeweX/RotatingFeatures/blob/main/imgs/RotatingFeatures.png?raw=true" alt="Graphical Abstract" style="height: auto; max-height: 90%; width: auto; max-width: 90%; margin-left: 5%;" />
      <span style="position: absolute; top: 14%; right: 0; background-color: #ffcc00; color: black; padding: 5px; font-size: 20px; transform: rotate(35deg); transform-origin: top right;">
        Oral Presentation
      </span>
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;" />

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Deep Gaussian Markov Random Fields for graph-structured dynamical systems</strong><br />
      <em>Fiona Lippert, Bart Kranstauber, E. Emiel van Loon, Patrick Forré</em><br />

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://arxiv.org/abs/2306.08445" style="text-decoration: none; color: #007bff;">Conference paper</a> &middot; 
        <a href="https://arxiv.org/pdf/2306.08445.pdf" style="text-decoration: none; color: #007bff;">PDF</a>
      </span>

      <strong>Abstract:</strong> Probabilistic inference in high-dimensional state-space models is computationally challenging. For many spatiotemporal systems, however, prior knowledge about the dependency structure of state variables is available. We leverage this structure to develop a computationally efficient approach to state estimation and learning in graph-structured state-space models with (partially) unknown dynamics and limited historical data. Building on recent methods that combine ideas from deep learning with principled inference in Gaussian Markov random fields (GMRF), we reformulate graph-structured state-space models as Deep GMRFs defined by simple spatial and temporal graph layers. This results in a flexible spatiotemporal prior that can be learned efficiently from a single time sequence via variational inference. Under linear Gaussian assumptions, we retain a closed-form posterior, which can be sampled efficiently using the conjugate gradient method, scaling favourably compared to classical Kalman filter based approaches. <br />

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #GaussianMarkovRandomFields #SpatiotemporalModels #VariationalInference
      </span>

    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/ST-DGMRF_graphical_abstract.png" alt="Graphical Abstract" style="height: auto; max-height: 80%; width: auto; max-width: 80%;  margin-left: 20%;" />
    </td>
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;" />

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Implicit Convolutional Kernels for Steerable CNNs</strong><br />
      <em>Maksim Zhdanov, Nico Hoffmann, Gabriele Cesa</em><br />
      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
    <a href="https://openreview.net/forum?id=2YtdxqvdjX" style="text-decoration: none; color: #007bff;">Conference paper</a> &middot; 
    <a href="https://openreview.net/pdf?id=2YtdxqvdjX" style="text-decoration: none; color: #007bff;">PDF</a> &middot; 
    <a href="https://github.com/maxxxzdn/implicit-steerable-cnns" style="text-decoration: none; color: #007bff;">GitHub repo</a>
  </span>

  <strong>Abstract:</strong> Steerable convolutional neural networks (CNNs) provide a general framework for building neural networks equivariant to translations and other transformations belonging to an origin-preserving group \(G\), such as reflections and rotations. They rely on standard convolutions with \(G\)-steerable kernels obtained by analytically solving the group-specific equivariance constraint imposed onto the kernel space. As the solution is tailored to a particular group \(G\), the implementation of a kernel basis does not generalize to other symmetry transformations, which complicates the development of general group equivariant models. We propose using implicit neural representation via multi-layer perceptrons (MLPs) to parameterize \(G\)-steerable kernels. The resulting framework offers a simple and flexible way to implement Steerable CNNs and generalizes to any group \(G\) for which a \(G\)-equivariant MLP can be built. We prove the effectiveness of our method on multiple tasks, including N-body simulations, point cloud classification, and molecular property prediction. <br />

  <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
    #SteerableCNNs #Equivariance #MolecularSimulations
  </span>

</td>
    <td style="width: 50%;">
      <img src="https://github.com/maxxxzdn/implicit-steerable-kernels/blob/main/assets/abstract.png?raw=true" alt="Graphical Abstract" style="height: auto; max-height: 60%; width: auto; max-width: 60%; margin-left: 20%;" />
    </td>

  </tr>
</table>
<hr style="margin-top: 20px;" />

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">The Memory-Perturbation Equation: Understanding Model's Sensitivity to Data</strong><br />
      <em>Peter Nickl, Lu Xu*, Dharmesh Tailor*, Thomas Möllenhoff, Emtiyaz Khan</em><br /><em>(*=equal contribution)</em><br />

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://openreview.net/forum?id=dqS1GuoG2V" style="text-decoration: none; color: #007bff;">Conference paper</a> &middot; 
        <a href="https://arxiv.org/abs/2310.19273" style="text-decoration: none; color: #007bff;">PDF</a> &middot; 
        <a href="https://github.com/team-approx-bayes/memory-perturbation" style="text-decoration: none; color: #007bff;">GitHub repo</a>
      </span>

      <strong>Abstract:</strong> Understanding model's sensitivity to its training data is crucial but can also be challenging and costly, especially during training. To simplify such issues, we present the Memory-Perturbation Equation (MPE) which relates model's sensitivity to perturbation in its training data. Derived using Bayesian principles, the MPE unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. Our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data. The proposed equation is expected to be useful for future research on robust and adaptive learning. <br />

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #MemoryPerturbationEquation #Sensitivity #Generalization
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/mpe_blog_image.png" alt="Graphical Abstract" style="height: auto; max-height: 60%; width: auto; max-width: 60%;  margin-left: 20%;" />
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;" />

<table>

  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Topological Obstructions and How to Avoid Them</strong><br />
      <em>Babak Esmaeili, Robin Walters, Heiko Zimmermann, Jan-Willem van de Meent</em><br />

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://openreview.net/forum?id=1tviRBNxI9" style="text-decoration: none; color: #007bff;">Conference paper</a> &middot; 
        <a href="https://openreview.net/pdf?id=1tviRBNxI9" style="text-decoration: none; color: #007bff;">PDF</a> &middot; 
        <!-- GitHub link not provided -->
      </span>

      <strong>Abstract:</strong> Incorporating geometric inductive biases into models can aid interpretability and generalization, but encoding to a specific geometric structure can be challenging due to the imposed topological constraints. In this paper, we theoretically and empirically characterize obstructions to training encoders with geometric latent spaces. We show that local optima can arise due to singularities (e.g. self-intersection) or due to an incorrect degree or winding number. We then discuss how normalizing flows can potentially circumvent these obstructions by defining multimodal variational distributions. Inspired by this observation, we propose a new flow-based model that maps data points to multimodal distributions over geometric spaces and empirically evaluate our model on 2 domains. We observe improved stability during training and a higher chance of converging to a homeomorphic encoder. <br />

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #GeometricInductiveBiases #DataTopology #NormalizingFlows
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/top_obst.jpeg" alt="Graphical Abstract" style="height: auto; max-height: 90%; width: auto; max-width: 90%;  margin-left: 10%;" />
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;" />

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning</strong><br />
      <em>Fan Feng, Sara Magliacane</em><br />

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://arxiv.org/abs/2307.09205" style="text-decoration: none; color: #007bff;">Paper</a> &middot; 
        <a href="https://arxiv.org/pdf/2307.09205.pdf" style="text-decoration: none; color: #007bff;">PDF</a> &middot; 
        <!-- GitHub repo to be updated later -->
      </span>

      <strong>Abstract:</strong> In many reinforcement learning tasks, the agent has to learn to interact with many objects of different types and generalize to unseen combinations and numbers of objects. Often a task is a composition of previously learned tasks (e.g. block stacking). These are examples of compositional generalization, in which we compose object-centric representations to solve complex tasks. Recent works have shown the benefits of object-factored representations and hierarchical abstractions for improving sample efficiency in these settings. In this paper, we introduce the Dynamic Attribute FacTored RL (DAFT-RL) framework. In DAFT-RL, we leverage object-centric representation learning to extract objects from visual inputs, classify them in classes, and infer their latent parameters. For each class of object, we learn a class template graph that describes the dynamics and reward of an object of this class factorized according to its attributes. We also learn an interaction pattern graph that describes how objects of different classes interact with each other at the attribute level. Through these graphs and a dynamic interaction graph that models the interactions between objects, we learn a policy that can then be directly applied in a new environment by just estimating the interactions and latent parameters. We evaluate DAFT-RL in three benchmark datasets and show our framework outperforms the state-of-the-art in generalizing across unseen objects with varying attributes and latent parameters, as well as in the composition of previously learned tasks. <br />

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #ReinforcementLearning #ObjectCentricRepresentations
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/dyn_attr.png" alt="Graphical Abstract" style="height: auto; max-height: 100%; width: auto; max-width: 100%;  margin-left: 10%;" />
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;" />

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity</strong><br />
      <em>Metod Jazbec, James Urquhart Allingham, Dan Zhang, Eric Nalisnick</em><br />

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://arxiv.org/abs/2306.02652" style="text-decoration: none; color: #007bff;">Paper</a> &middot; 
        <a href="https://github.com/metodj/AnytimeClassification" style="text-decoration: none; color: #007bff;">GitHub repo</a>
      </span>

      <strong>Abstract:</strong> Modern predictive models are often deployed to environments in which computational budgets are dynamic. Anytime algorithms are well-suited to such environments as, at any point during computation, they can output a prediction whose quality is a function of computation time. Early-exit neural networks have garnered attention in the context of anytime computation due to their capability to provide intermediate predictions at various stages throughout the network. However, we demonstrate that current early-exit networks are not directly applicable to anytime settings, as the quality of predictions for individual data points is not guaranteed to improve with longer computation. To address this shortcoming, we propose an elegant post-hoc modification, based on the Product-of-Experts, that encourages an early-exit network to become gradually confident. This gives our deep models the property of conditional monotonicity in the prediction quality -- an essential stepping stone towards truly anytime predictive modeling using early-exit architectures. Our empirical results on standard image-classification tasks demonstrate that such behaviors can be achieved while preserving competitive accuracy on average. <br />

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #AnytimeClassification #EarlyExitNetworks #ImageClassification
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/PA_twitter_v2.png" alt="Graphical Abstract" style="height: auto; max-height: 80%; width: auto; max-width: 80%;  margin-left: 10%;" />
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;" />

<div style="background-color: rgba(123, 104, 238, 0.1); padding: 10px 0; text-align: center;">
    <h2 style="color: #524364; font-weight: bold;">Workshops</h2>
    <img src="https://neurips.cc/media/Press/NeurIPS_logo.svg" alt="Workshops" width="180" height="90" style="margin: auto;" />
</div>

<hr style="margin-top: 20px;" />

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Switching policies for solving inverse problems</strong><br />
      <em>Tim Bakker, Fabio Valerio Massoli, Thomas Hehn, Tribhuvanesh Orekondy, Arash Behboodi</em><br />

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://openreview.net/forum?id=QaIEU0wWj8" style="text-decoration: none; color: #007bff;">Conference paper</a> &middot; 
        <a href="https://openreview.net/pdf?id=QaIEU0wWj8" style="text-decoration: none; color: #007bff;">PDF</a> &middot; 
        <!-- GitHub repo not provided -->
      </span>

      <strong>Abstract:</strong> In recent years, inverse problems for black-box simulators have enjoyed increased focus of the machine learning community due to their prevalence in science and engineering domains. Such simulators describe a forward process \(f: (\psi, x) \rightarrow y\). Here the intent is to optimise simulator parameters \(\psi\) to minimise some observation loss on \(y\), under some input distribution on \(x\). Optimisation of such objectives is often challenging, since it is not trivial to estimate simulator gradients accurately. In settings where multiple related inverse problems need to be solved simultaneously, from-scratch/ab-initio optimisation of each may be infeasible if the forward model is expensive to evaluate. In this paper, we propose a novel method for solving such families of inverse problems with reinforcement learning. We train a policy to guide the optimisation by selecting between gradients estimated numerically from the simulator and gradients estimated from a pre-trained surrogate model. After training the surrogate and the policy, downstream inverse problem optimisations require 10%-70% fewer simulator evaluations. Moreover, the policy does successful optimisations on functions where using just simulator gradient estimates fails.
 <br />

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #InverseProblems #ReinforcementLearning
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/tim2.png" alt="Graphical Abstract" style="height: auto; max-height: 80%; width: auto; max-width: 80%;  margin-left: 10%;" />
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;" />

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Active Learning Policies for Solving Inverse Problems</strong><br />
      <em>Tim Bakker, Thomas Hehn, Tribhuvanesh Orekondy, Arash Behboodi, Fabio Valerio Massoli</em><br />

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://openreview.net/forum?id=eLIk3m5C79" style="text-decoration: none; color: #007bff;">Conference paper</a>
        <!-- PDF and GitHub links not provided -->
      </span>

      <strong>Abstract:</strong> In recent years, solving inverse problems for black-box simulators has become a point of focus for the machine learning community due to their ubiquity in science and engineering scenarios. In such settings, the simulator describes a forward process \(f: (\psi, x) \rightarrow y\) from simulator parameters \(\psi\) and input data \(x\) to observations \(y\), and the goal of the inverse problem is to optimise \(\psi\) to minimise some observation loss. Simulator gradients are often unavailable or prohibitively expensive to obtain, making optimisation of these simulators particularly challenging. Moreover, in many applications, the goal is to solve a family of related inverse problems. Thus, starting optimisation ab-initio/from-scratch may be infeasible if the forward model is expensive to evaluate. In this paper, we propose a novel method for solving classes of similar inverse problems. We learn an active learning policy that guides the training of a surrogate and use the gradients of this surrogate to optimise the simulator parameters with gradient descent. After training the policy, downstream inverse problem optimisations require up to 90\% fewer forward model evaluations than the baseline. <br />

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #InverseProblems #ActiveLearning
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/active.png" alt="Graphical Abstract" style="height: auto; max-height: 80%; width: auto; max-width: 80%;  margin-left: 10%;" />
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;" />

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Designing Long-term Group Fair Policies in Dynamical Systems</strong><br />
      <em>Miriam Rateike, Isabel Valera, Patrick Forré</em><br />

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://arxiv.org/abs/2311.12447" style="text-decoration: none; color: #007bff;">Paper</a> &middot; 
        <a href="https://arxiv.org/pdf/2311.12447.pdf" style="text-decoration: none; color: #007bff;">PDF</a> &middot; 
        <!-- GitHub repo not provided -->
      </span>

      <strong>Abstract:</strong> This paper proposes a novel framework for achieving long-term group fairness in dynamical systems. The framework aims to identify a time-independent policy that ensures convergence to a fair stationary state, regardless of the initial data distribution. By modeling system dynamics with a time-homogeneous Markov chain and leveraging the Markov chain convergence theorem, the paper provides examples of various long-term fair states and demonstrates how the approach can evaluate the impact of different long-term targets on group-conditional population distribution over time. <br />

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #LongTermFairness #DynamicalSystems #MarkovChains
      </span>

      </td>
      <td style="position: relative; width: 50%;">
        <img src="/assets/neurips2023/Long-term-fair-picture-abstract.png" alt="Graphical Abstract" style="height: auto; max-height: 70%; width: auto; max-width: 70%;  margin-left: 15%;" />
        <span style="position: absolute; top: 14%; right: 0; background-color: #ffcc00; color: black; padding: 5px; font-size: 20px; transform: rotate(35deg); transform-origin: top right;">
          Oral Presentation
        </span>
      </td>
  </tr>
</table>

<hr style="margin-top: 20px;" />]]></content><author><name>Max Zhdanov</name></author><summary type="html"><![CDATA[Read here about the 8 papers that AMLab members will be presenting at NeurIPS this year.]]></summary></entry><entry><title type="html">AI alignment blog</title><link href="https://amlab-amsterdam.github.io/blog/2023/TimAlignment/" rel="alternate" type="text/html" title="AI alignment blog" /><published>2023-10-27T07:00:00+00:00</published><updated>2023-10-27T07:00:00+00:00</updated><id>https://amlab-amsterdam.github.io/blog/2023/TimAlignment</id><content type="html" xml:base="https://amlab-amsterdam.github.io/blog/2023/TimAlignment/"><![CDATA[<p>Tim Bakker’s writings on AI alignment.</p>

<p>Posts:</p>
<ol>
  <li>
    <p><a href="https://www.tbbakker.nl/post/2023_05_alignment/">A comprehensive primer on AI alignment.</a></p>
  </li>
  <li>
    <p><a href="https://www.tbbakker.nl/post/2023_10_optimisation/">Viewing AI risk through the lens of optimisation.</a></p>
  </li>
</ol>]]></content><author><name>Tim Bakker</name></author><summary type="html"><![CDATA[What is AI alignment, why is it important, and how can we achieve it?]]></summary></entry><entry><title type="html">Blogging on Bayes</title><link href="https://amlab-amsterdam.github.io/blog/2023/TimBayes/" rel="alternate" type="text/html" title="Blogging on Bayes" /><published>2023-05-03T07:00:00+00:00</published><updated>2023-05-03T07:00:00+00:00</updated><id>https://amlab-amsterdam.github.io/blog/2023/TimBayes</id><content type="html" xml:base="https://amlab-amsterdam.github.io/blog/2023/TimBayes/"><![CDATA[<p>Tim Bakker’s musings on Bayesianism, following the AMLab reading group on E.T. Jaynes’ book <em><a href="https://bayes.wustl.edu/etj/prob/book.pdf">Probability Theory: The Logic of Science</a></em>.</p>

<p>Posts:</p>
<ol>
  <li>
    <p><a href="https://www.tbbakker.nl/post/2021_03_bayes_commentary/">A look at the objective Bayesian prescription for reasoning under uncertainty.</a></p>
  </li>
  <li>
    <p><a href="https://www.tbbakker.nl/post/2021_04_bayes2ml/">A tour from prescriptive Bayesianism to modern statistical machine learning.</a></p>
  </li>
  <li>
    <p><a href="https://www.tbbakker.nl/post/2023_05_ml_priors/">Exploring the role of Bayesian priors in modern deep learning.</a></p>
  </li>
</ol>]]></content><author><name>Tim Bakker</name></author><summary type="html"><![CDATA[Musings on the objective Bayesian prescription for reasoning under uncertainty and statistical machine learning.]]></summary></entry><entry><title type="html">AMLab Papers at ICML 2022</title><link href="https://amlab-amsterdam.github.io/blog/2022/icml/" rel="alternate" type="text/html" title="AMLab Papers at ICML 2022" /><published>2022-07-19T07:00:00+00:00</published><updated>2022-07-19T07:00:00+00:00</updated><id>https://amlab-amsterdam.github.io/blog/2022/icml</id><content type="html" xml:base="https://amlab-amsterdam.github.io/blog/2022/icml/"><![CDATA[<p><img src="https://icml.cc/static/core/img/ICML-logo.svg" alt="ICML 2022" /></p>

<p>AMLab members will be presenting 8 papers at ICML 2022 main conference:</p>

<ol>
  <li>
    <p><strong>Equivariant diffusion for molecule generation in 3d (oral)</strong> <br />
<em>Hoogeboom, Emiel, Satorras, Vı́ctor Garcia, Vignac, Clément, and Welling, Max</em> <br />
[<a href="https://icml.cc/virtual/2022/oral/16254">Conference page</a>, <a href="https://arxiv.org/pdf/2203.17003">PDF</a>, <a href="https://github.com/ehoogeboom/e3_diffusion_for_molecules">GitHub</a>]</p>
  </li>
  <li>
    <p><strong>Lie Point Symmetry Data Augmentation for Neural PDE Solvers (spotlight)</strong> <br />
<em>Brandstetter, Johannes, Welling, Max, and Worrall, Daniel E</em> <br />
[<a href="https://icml.cc/virtual/2022/spotlight/17314">Conference page</a>, <a href="https://arxiv.org/pdf/2202.07643">PDF</a>, <a href="https://github.com/brandstetter-johannes/LPSDA">GitHub</a>]</p>
  </li>
  <li>
    <p><strong>Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups (spotlight)</strong> <br />
<em>Knigge, David M, Romero, David W, and Bekkers, Erik J</em><br />
[<a href="https://icml.cc/virtual/2022/spotlight/17826">Conferende page</a>, <a href="https://arxiv.org/pdf/2110.13059.pdf">PDF</a>, <a href="https://github.com/david-knigge/separable-group-convolutional-networks">GitHub</a>]</p>
  </li>
  <li>
    <p><strong>CITRIS: Causal Identifiability from Temporal Intervened Sequences (spotlight)</strong> <br />
<em>Lippe, Phillip, Magliacane, Sara, Löwe, Sindy, Asano, Yuki M, Cohen, Taco, and Gavves, Efstratios</em> <br />
[<a href="https://icml.cc/virtual/2022/spotlight/17426">Conferende page</a>, <a href="https://arxiv.org/pdf/2202.03169.pdf">PDF</a>, <a href="https://github.com/phlippe/CITRIS">GitHub</a>]</p>
  </li>
  <li>
    <p><strong>Learning Symmetric Embeddings for Equivariant World Models (spotlight)</strong> <br />
<em>Park, Jung Yeon, Biza, Ondrej, Zhao, Linfeng, Meent, Jan-Willem, and Walters, Robin</em> <br />
[<a href="https://icml.cc/virtual/2022/spotlight/17396">Conferende page</a>, <a href="https://arxiv.org/pdf/2204.11371.pdf">PDF</a>]</p>
  </li>
  <li>
    <p><strong>Adapting the Linearised Laplace Model Evidence for Modern Deep Learning (spotlight)</strong> <br />
<em>Antoran, Javier, Janz, David, Allingham, James Urquhart, Daxberger, Erik, Barbano, Riccardo, Nalisnick, Eric, and Hernandez-Lobato, Jose Miguel</em> <br />
[<a href="https://icml.cc/virtual/2022/spotlight/18290">Conferende page</a>, <a href="https://edaxberger.github.io/papers/Adapting%20the%20Linearised%20Laplace%20Model%20Evidence%20for%20Modern%20Deep%20Learning.pdf">PDF</a>]</p>
  </li>
  <li>
    <p><strong>Calibrated Learning to Defer with One-vs-All Classifiers (spotlight)</strong> <br />
<em>Verma, Rajeev, and Nalisnick, Eric</em> <br />
[<a href="https://icml.cc/virtual/2022/spotlight/18124">Conferende page</a>, <a href="https://arxiv.org/pdf/2202.03673">PDF</a>]</p>
  </li>
  <li>
    <p><strong>Model-based Meta Reinforcement Learning using Graph Structured Surrogate Models and Amortized Policy Search (spotlight)</strong> <br />
<em>Wang, Q., and Hoof, H.</em> <br />
[<a href="https://icml.cc/virtual/2022/spotlight/17730">Conferende page</a>, <a href="https://arxiv.org/abs/2102.08291">PDF</a>]</p>
  </li>
</ol>]]></content><author><name>Erik Bekkers</name></author><summary type="html"><![CDATA[Read here about the 8 papers that AMLab members will be presenting at ICML this year.]]></summary></entry><entry><title type="html">AMLab Papers at ICLR 2022</title><link href="https://amlab-amsterdam.github.io/blog/2022/iclr/" rel="alternate" type="text/html" title="AMLab Papers at ICLR 2022" /><published>2022-04-25T07:00:00+00:00</published><updated>2022-04-25T07:00:00+00:00</updated><id>https://amlab-amsterdam.github.io/blog/2022/iclr</id><content type="html" xml:base="https://amlab-amsterdam.github.io/blog/2022/iclr/"><![CDATA[<p><img src="https://iclr.cc/static/core/img/ICLR-logo.svg" alt="ICLR 2022" /></p>

<p>AMLab members will be presenting 6 papers at ICLR 2022 main conference:</p>

<ol>
  <li>
    <p><strong>Geometric and Physical Quantities improve E(3) Equivariant Message Passing (spotlight)</strong> <br />
<em>Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, Max Welling</em> <br />
[<a href="https://openreview.net/forum?id=_xwr8gOBeV1">OpenReview</a>, <a href="https://iclr.cc/virtual/2022/poster/6225">Poster Session</a>, <a href="https://robdhess.github.io/Steerable-E3-GNN/">Blog</a>]</p>
  </li>
  <li>
    <p><strong>Message Passing Neural PDE Solvers (spotlight)</strong> <br />
<em>Johannes Brandstetter, Daniel E. Worrall, Max Welling</em><br />
[<a href="https://openreview.net/forum?id=vSix3HPYKSU">OpenReview</a>, <a href="https://iclr.cc/virtual/2022/poster/7134">Poster Session</a>]</p>
  </li>
  <li>
    <p><strong>Multi-Agent MDP Homomorphic Networks</strong> <br />
<em>Elise van der Pol, Herke van Hoof, Frans A Oliehoek, Max Welling</em> <br />
[<a href="https://openreview.net/forum?id=H7HDG--DJF0">OpenReview</a>, <a href="https://iclr.cc/virtual/2022/poster/6821">Poster Session</a>]</p>
  </li>
  <li>
    <p><strong>FlexConv: Continuous Kernel Convolutions With Differentiable Kernel Sizes</strong> <br />
<em>David W. Romero, Robert-Jan Bruintjes, Jakub Mikolaj Tomczak, Erik J Bekkers, Mark Hoogendoorn, Jan van Gemert</em> <br />
[<a href="https://openreview.net/forum?id=3jooF27-0Wy">OpenReview</a>, <a href="https://iclr.cc/virtual/2022/poster/6763">Poster Session</a>]</p>
  </li>
  <li>
    <p><strong>CKConv: Continuous Kernel Convolution For Sequential Data</strong> <br />
<em>David W. Romero, Anna Kuzina, Erik J Bekkers, Jakub Mikolaj Tomczak, Mark Hoogendoorn</em> <br />
[<a href="https://openreview.net/forum?id=8FhxBtXSl0">OpenReview</a>, <a href="https://iclr.cc/virtual/2022/poster/6393">Poster Session</a>]</p>
  </li>
  <li>
    <p><strong>Self-Supervised Inference in State-Space Models</strong> <br />
<em>David Ruhe, Patrick Forré</em> <br />
[<a href="https://openreview.net/forum?id=VPjw9KPWRSK">OpenReview</a>, <a href="https://iclr.cc/virtual/2022/poster/6606">Poster Session</a>]</p>
  </li>
</ol>]]></content><author><name>Jan-Willem van de Meent</name></author><summary type="html"><![CDATA[Read here about the 6 papers that AMLab members will be presenting at ICLR this year.]]></summary></entry><entry><title type="html">Steerable E(3)-Equivariant Graph Neural Networks</title><link href="https://amlab-amsterdam.github.io/blog/2022/segnn/" rel="alternate" type="text/html" title="Steerable E(3)-Equivariant Graph Neural Networks" /><published>2022-04-05T07:00:00+00:00</published><updated>2022-04-05T07:00:00+00:00</updated><id>https://amlab-amsterdam.github.io/blog/2022/segnn</id><content type="html" xml:base="https://amlab-amsterdam.github.io/blog/2022/segnn/"><![CDATA[<p>A new <a href="https://arxiv.org/abs/2110.02905">publication</a> by Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers and Max Welling introduces the steerable E(3)-equivariant graph neural network, or SEGNN. This model uses irreducible representations of the orthogonal group O(3) to condition messages on relative positions and other geometric attributes and achieves excellent results on QM9 and the Open Catalyst Project. Accompanying the paper is a <a href="https://robdhess.github.io/Steerable-E3-GNN/">blog post</a> that explains how to build O(3) steerable networks and showcases some of the benefits of equivariant message passing. The paper was awarded a spotlight at ICLR 2022.</p>

<p><img src="https://raw.githubusercontent.com/RobDHess/Steerable-E3-GNN/gh-pages/assets/forward_pass_faster_larger.gif" alt="SEGNN" /></p>]]></content><author><name>Rob Hesselink</name></author><summary type="html"><![CDATA[Read about our work on E(3)-steerable graph neural networks for molecular data.]]></summary></entry><entry><title type="html">Putting an End to End-to-End</title><link href="https://amlab-amsterdam.github.io/blog/2020/GreedyInfoMax/" rel="alternate" type="text/html" title="Putting an End to End-to-End" /><published>2020-04-12T07:00:00+00:00</published><updated>2020-04-12T07:00:00+00:00</updated><id>https://amlab-amsterdam.github.io/blog/2020/GreedyInfoMax</id><content type="html" xml:base="https://amlab-amsterdam.github.io/blog/2020/GreedyInfoMax/"><![CDATA[<p>In our paper <a href="https://arxiv.org/abs/1905.11786">Putting An End to End-to-End: Gradient-Isolated Learning of Representations (S. Löwe, P. O’Connor, B. S. Veeling)</a>, we showed that we can train a neural network without end-to-end backpropagation and achieve competitive performance. For this, we received an Honorable Mention for Outstanding New Directions Paper Award at NeurIPS 2019.</p>

<p>Find out more about this paper in <strong><a href="https://loewex.github.io/GreedyInfoMax.html">this blog-post</a></strong> by Sindy Löwe.</p>]]></content><author><name>Sindy Löwe</name></author><summary type="html"><![CDATA[Find out more about how we can train a neural network without end-to-end backpropagation and achieve competitive performance.]]></summary></entry></feed>