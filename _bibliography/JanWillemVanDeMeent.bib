@inproceedings{matisan2026purrception,
  title = {Purrception: Variational Flow Matching for Vector-Quantized Image Generation},
  author = {Mati{\c{s}}an, R{\u{a}}zvan-Andrei and Hu, Vincent Tao and Bartosh, Grigory and Ommer, Bj{\"o}rn and Snoek, Cees G. M. and Welling, Max and {van de Meent}, Jan-Willem and Derakhshani, Mohammad Mahdi and Eijkelboom, Floor},
  booktitle = {International Conference on Learning Representations},
  year = {2026},
  abbr = {ICLR},
  html = {https://openreview.net/forum?id=SA8xDYrUYB},
  pdf = {https://arxiv.org/pdf/2510.01478}
}

@inproceedings{hadziveljkovic2026cords,
  title = {{CORDS}: {C}ontinuous {R}epresentations of {D}iscrete {S}tructures},
  author = {{Had\v{z}i Veljkovi\'{c}}, Tin and Bekkers, Erik J. and Tiemann, Michael and {van de Meent}, Jan-Willem},
  booktitle = {International Conference on Learning Representations},
  year = {2026},
  abbr = {ICLR},
  html = {https://openreview.net/forum?id=RObkOKADBU},
  pdf = {https://arxiv.org/pdf/2601.21583}
}

@inproceedings{zaghen2026riemannian,
  title = {Riemannian Variational Flow Matching for Material and Protein Design},
  author = {Zaghen, Olga and Eijkelboom, Floor and Pouplin, Alison and Liu, Cong and Welling, Max and {van de Meent}, Jan-Willem and Bekkers, Erik J.},
  booktitle = {International Conference on Learning Representations},
  year = {2026},
  abbr = {ICLR},
  html = {https://openreview.net/forum?id=NlnDselrtl},
  pdf = {https://arxiv.org/pdf/2502.12981}
}

@article{curvo2025mspt,
  title = {MSPT: Efficient Large-Scale Physical Modeling via Parallelized Multi-Scale Attention},
  author = {Curvo, Pedro M. P. and {van de Meent}, Jan-Willem and Zhdanov, Maksim},
  year = {2025},
  journal = {arXiv preprint},
  abbr = {arXiv},
  html = {https://arxiv.org/abs/2512.01738},
  pdf = {https://arxiv.org/pdf/2512.01738}
}

@article{park2025liegroups,
  title = {Discovering Lie Groups with Flow Matching},
  author = {Park, Jung Yeon and Chen, Yuxuan and Eijkelboom, Floor and {van de Meent}, Jan-Willem and Wong, Lawson L. S. and Walters, Robin},
  year = {2025},
  journal = {arXiv preprint},
  abbr = {arXiv},
  html = {https://arxiv.org/abs/2512.20043},
  pdf = {https://arxiv.org/pdf/2512.20043}
}

@article{farnoosh2020deep,
  title = {Deep {{Markov Spatio}}-{{Temporal Factorization}}},
  author = {Farnoosh, Amirreza and Rezaei, Behnaz and Sennesh, Eli Zachary and Khan, Zulqarnain and Dy, Jennifer and Satpute, Ajay and Hutchinson, J. Benjamin and {van de Meent}, Jan-Willem and Ostadabbas, Sarah},
  year = {2020},
  month = mar,
  abstract = {We introduce deep Markov spatio-temporal factorization (DMSTF), a deep generative model for spatio-temporal data. Like other factor analysis methods, DMSTF approximates high-dimensional data by a product between time-dependent weights and spatially dependent factors. These weights and factors are in turn represented in terms of lower-dimensional latent variables that we infer using stochastic variational inference. The innovation in DMSTF is that we parameterize weights in terms of a deep Markovian prior, which is able to characterize nonlinear temporal dynamics. We parameterize the corresponding variational distribution using a bidirectional recurrent network. This results in a flexible family of hierarchical deep generative factor analysis models that can be extended to perform time series clustering, or perform factor analysis in the presence of a control signal. Our experiments, which consider simulated data, fMRI data, and traffic data, demonstrate that DMSTF outperforms related methods in terms of reconstruction accuracy and can perform forecasting in a variety domains with nonlinear temporal transitions.},
  archivePrefix = {arXiv},
  eprint = {2003.09779},
  eprinttype = {arxiv},
  journal = {arXiv:2003.09779 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat},
  html = {https://arxiv.org/abs/2003.09779},
  pdf = {https://arxiv.org/pdf/2003.09779},
  abbr = {arXiv}
}

@article{vandemeent2018introduction,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.10756},
  primaryClass = {cs, stat},
  title = {An {{Introduction}} to {{Probabilistic Programming}}},
  abstract = {This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.},
  journal = {arXiv:1809.10756 [cs, stat]},
  author = {{van de Meent}, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
  month = sep,
  year = {2018},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Programming Languages,Computer Science - Machine Learning},
  html = {https://arxiv.org/abs/1809.10756},
  pdf = {https://arxiv.org/pdf/1809.10756},
  abbr = {arXiv}
}

@inproceedings{eijkelboom2025controlled,
  title = {Controlled Generation with Equivariant Variational Flow Matching},
  author = {Eijkelboom, Floor and Zimmermann, Heiko and Bekkers, Erik and Welling, Max and Naesseth, Christian and {van de Meent}, Jan-Willem},
  booktitle = {International Conference on Machine Learning},
  year = {2025},
  abbr = {ICML},
  html = {https://openreview.net/forum?id=YSVSMV0lXQ},
  pdf = {https://openreview.net/pdf?id=YSVSMV0lXQ}
}

@inproceedings{guzmancordero2025exponential,
  title = {Exponential Family Variational Flow Matching for Tabular Data Generation},
  author = {{Guzm\'an-Cordero}*, Andr\'es and Eijkelboom*, Floor and {van de Meent}, Jan-Willem},
  booktitle = {International Conference on Machine Learning},
  year = {2025},
  abbr = {ICML},
  html = {https://openreview.net/forum?id=kjtvCSkSsy},
  pdf = {https://openreview.net/pdf?id=kjtvCSkSsy}
}

@inproceedings{zhdanov2025erwin,
  title = {Erwin: {{A Tree-based Hierarchical Transformer}} for {{Large-scale Physical Systems}}},
  shorttitle = {Erwin},
  author = {Zhdanov, Maksim and Welling, Max and {van de Meent}, Jan-Willem},
  booktitle = {International Conference on Machine Learning},
  year = {2025},
  number = {arXiv:2502.17019},
  eprint = {2502.17019},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.17019},
  urldate = {2025-05-06},
  abstract = {Large-scale physical systems defined on irregular grids pose significant scalability challenges for deep learning methods, especially in the presence of long-range interactions and multi-scale coupling. Traditional approaches that compute all pairwise interactions, such as attention, become computationally prohibitive as they scale quadratically with the number of nodes. We present Erwin, a hierarchical transformer inspired by methods from computational many-body physics, which combines the efficiency of tree-based algorithms with the expressivity of attention mechanisms. Erwin employs ball tree partitioning to organize computation, which enables linear-time attention by processing nodes in parallel within local neighborhoods of fixed size. Through progressive coarsening and refinement of the ball tree structure, complemented by a novel cross-ball interaction mechanism, it captures both fine-grained local details and global features. We demonstrate Erwin's effectiveness across multiple domains, including cosmology, molecular dynamics, and particle fluid dynamics, where it consistently outperforms baseline methods both in accuracy and computational efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  abbr = {ICML},
  html = {https://arxiv.org/abs/2502.17019},
  pdf = {https://arxiv.org/pdf/2502.17019},
  code = {https://github.com/maxxxzdn/erwin}
}

@article{dijkman2025learning,
  title = {Learning {{Neural Free-Energy Functionals}} with {{Pair-Correlation Matching}}},
  author = {Dijkman, Jacobus and Dijkstra, Marjolein and {van Roij}, Ren{\'e} and Welling, Max and {van de Meent}, Jan-Willem and Ensing, Bernd},
  year = {2025},
  month = feb,
  journal = {Physical Review Letters},
  volume = {134},
  number = {5},
  pages = {056103},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.134.056103},
  urldate = {2025-05-06},
  abstract = {The intrinsic Helmholtz free-energy functional, the centerpiece of classical density functional theory, is at best only known approximately for 3D systems. Here we introduce a method for learning a neural-network approximation of this functional by exclusively training on a dataset of radial distribution functions, circumventing the need to sample costly heterogeneous density profiles in a wide variety of external potentials. For a supercritical Lennard-Jones system with planar symmetry, we demonstrate that the learned neural free-energy functional accurately predicts inhomogeneous density profiles under various complex external potentials obtained from simulations.},
  abbr = {PRL},
  html = {https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.134.056103},
  pdf = {https://arxiv.org/pdf/2403.15007}
}

@inproceedings{biza25onrobot,
  author       = {Ondrej Biza and
                  Thomas Weng and
                  Lingfeng Sun and
                  Karl Schmeckpeper and
                  Tarik Kelestemur and
                  Yecheng Jason Ma and
                  Robert Platt and
                  Jan{-}Willem {van de Meent} and
                  Lawson L. S. Wong},
  title        = {On-Robot Reinforcement Learning with Goal-Contrastive Rewards},
  booktitle    = {Proceedings of the 2025 IEEE International Conference on Robotics and Automation, ICRA'25},
  year         = {2025},
  abbr         = {ICRA},
  html         = {https://arxiv.org/abs/2410.19989},
  pdf          = {https://arxiv.org/pdf/2410.19989}
}

@inproceedings{kunze2024practical,
  title = {Practical Shuffle Coding},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kunze, Julius and Severo, Daniel and {van de Meent}, Jan-Willem and Townsend, James},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {84081--84113},
  abbr = {NeurIPS}, 
  publisher = {Curran Associates, Inc.},
  html = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/98d17a9632e1534bae96793e99dc3c2d-Abstract-Conference.html},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/98d17a9632e1534bae96793e99dc3c2d-Paper-Conference.pdf},
  code = {https://github.com/juliuskunze/shuffle-coding}
}

@inproceedings{eijkelboom2024variational,
  title = {Variational Flow Matching for Graph Generation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Eijkelboom, Floor and Bartosh, Grigory and Naesseth, Christian A. and Welling, Max and {van de Meent}, Jan-Willem},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {11735--11764},
  publisher = {Curran Associates, Inc.},
  abbr = {NeurIPS},
  html = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/15b780350b302a1bf9a3bd273f5c15a4-Abstract-Conference.html},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/15b780350b302a1bf9a3bd273f5c15a4-Paper-Conference.pdf}
}

@inproceedings{zimmermann2024visa,
  title = {{{VISA}}: {{Variational}} Inference with Sequential Sample-Average Approximations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zimmermann, Heiko and Naesseth, Christian A. and {van de Meent}, Jan-Willem},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {138789--138808},
  publisher = {Curran Associates, Inc.},
  abbr = {NeurIPS},
  html = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/fa948624dfde013671e72c1a7ca4aebc-Abstract-Conference.html},
  code = {https://github.com/zmheiko/visa},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/fa948624dfde013671e72c1a7ca4aebc-Paper-Conference.pdf}
}

@inproceedings{mcinerney2024reducing,
  title = {Towards {{Reducing Diagnostic Errors}} with {{Interpretable Risk Prediction}}},
  author = {McInerney, Denis Jered and Dickinson, William and Flynn, Lucy C. and Young, Andrea C. and Young, Geoffrey S. and {van de Meent}, Jan-Willem and Wallace, Byron C.},
  booktitle = {2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year = {2024},
  abstract = {Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model. We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses.},
  abbr = {NAACL},
  html = {https://aclanthology.org/2024.naacl-long.399/},
  pdf = {https://aclanthology.org/2024.naacl-long.399.pdf}
}


@inproceedings{kunze2024entropy,
    title={Entropy Coding of Unordered Data Structures},
    author={Julius Kunze and Daniel Severo and Giulio Zani and Jan-Willem {van de Meent} and James Townsend},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2024},
    abbr={ICLR},
    html={https://openreview.net/forum?id=PggJ9CbEN7},
    pdf={https://openreview.net/pdf?id=PggJ9CbEN7},
    code={https://github.com/juliuskunze/shuffle-coding}
}

@inproceedings{mcinerney2023chill,
  title={{CH}i{LL}: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models},
  author={McInerney, Denis Jered and Young, Geoffrey and van de Meent, Jan-Willem and Wallace, Byron},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  html={https://openreview.net/forum?id=TSdWY9GaHA},
  pdf={https://openreview.net/pdf?id=TSdWY9GaHA},
  abbr={EMNLP}
}

@inproceedings{
esmaeili2023topological,
title={Topological Obstructions and How to Avoid Them},
author={Esmaeili, Babak and Walters, Robin and Zimmermann, Heiko and {van de Meent}, {Jan-Willem}},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems (to appear)},
month={12},
year={2023},
html={https://proceedings.neurips.cc/paper_files/paper/2023/hash/1c12ccfc7720f6b680edea17300bfc2b-Abstract-Conference.html},
pdf={https://proceedings.neurips.cc/paper_files/paper/2023/file/1c12ccfc7720f6b680edea17300bfc2b-Paper-Conference.pdf},
abbr={NeurIPS}
}

@inproceedings{
biza2023oneshot,
title={One-shot Imitation Learning via Interaction Warping},
author={Ondrej Biza and Skye Thompson and Kishore Reddy Pagidi and Abhinav Kumar and Elise van der Pol and Robin Walters and Thomas Kipf and Jan-Willem van de Meent and Lawson L.S. Wong and Robert Platt},
booktitle={7th Annual Conference on Robot Learning},
month={11},
year={2023},
html={https://proceedings.mlr.press/v229/biza23a.html},
pdf={https://proceedings.mlr.press/v229/biza23a/biza23a.pdf},
abbr={CoRL}
}

@inproceedings{sennesh2023string,
  title = {String {{Diagrams}} with {{Factorized Densities}}},
  author = {Sennesh, Eli and {van de Meent}, Jan-Willem},
  year = {2023},
  month = {7},
  booktitle={Applied Category Theory},
  abstract = {A growing body of research on probabilistic programs and causal models has highlighted the need to reason compositionally about model classes that extend directed graphical models. Both probabilistic programs and causal models define a joint probability density over a set of random variables, and exhibit sparse structure that can be used to reason about causation and conditional independence. This work builds on recent work on Markov categories of probabilistic mappings to define a category whose morphisms combine a joint density, factorized over each sample space, with a deterministic mapping from samples to return values. This is a step towards closing the gap between recent category-theoretic descriptions of probability measures, and the operational definitions of factorized densities that are commonly employed in probabilistic programming and causal inference.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages,Mathematics - Category Theory,Mathematics - Probability},
  html = {https://openreview.net/forum?id=tfNdfCEWy2},
  pdf = {https://arxiv.org/pdf/2305.02506},
  abbr={ACT}
}

@article{zimmermann2023variational,
  title={A Variational Perspective on Generative Flow Networks},
  abstract = {Generative flow networks (GFNs) are a class of probabilistic models for sequential sampling of composite objects, proportional to a target distribution that is defined in terms of an energy function or a reward. GFNs are typically trained using a flow matching or trajectory balance objective, which matches forward and backward transition models over trajectories. In this work we introduce a variational objective for training GFNs, which is a convex combination of the reverse- and forward KL divergences, and compare it to the trajectory balance objective when sampling from the forward- and backward model, respectively. We show that, in certain settings, variational inference for GFNs is equivalent to minimizing the trajectory balance objective, in the sense that both methods compute the same score-function gradient. This insight suggests that in these settings, control variates, which are commonly used to reduce the variance of score-function gradient estimates, can also be used with the trajectory balance objective. We evaluate our findings and the performance of the proposed variational objective numerically by comparing it to the trajectory balance objective on two synthetic tasks.},
  author={Heiko Zimmermann and Fredrik Lindsten and Jan-Willem van de Meent and Christian A Naesseth},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  month={4},
  html={https://openreview.net/forum?id=AZ4GobeSLq},
  pdf={https://openreview.net/pdf?id=AZ4GobeSLq},
  code={https://github.com/zmheiko/variational-perspective-on-gflownets},
  abbr={TMLR}
}


@inproceedings{townsend_lafi_2023,
    author = {James Townsend and Jan-Willem {van de Meent}},
    booktitle = {POPL Workshop on Languages for Inference (LAFI)},
    title = {{Verified Reversible Programming for Verified Lossless Compression}},
    html = {https://popl23.sigplan.org/details/lafi-2023-papers/2/Verified-Reversible-Programming-for-Verified-Lossless-Compression},
    pdf = {https://arxiv.org/pdf/2211.09676.pdf},
    abbr = {LAFI},
    year = {2023}
}

@inproceedings{sennesh_lafi_2023,
    author = {Eli Sennesh and Jan-Willem {van de Meent}},
    booktitle = {POPL Workshop on Languages for Inference (LAFI)},
    title = {{A convenient category of tracing measure kernels}},
    pdf = {https://popl23.sigplan.org/details?action-call-with-get-request-type=1&e045187d36474ff29b18a21bf9aab73aaction_174265066101788833e464a9a6ab51286f3e5105d96=1&__ajax_runtime_request__=1&context=POPL-2023&track=lafi-2023-papers&urlKey=8&decoTitle=A-convenient-category-of-tracing-measure-kernels},
    html = {https://popl23.sigplan.org/details/lafi-2023-papers/8/A-convenient-category-of-tracing-measure-kernels},
    abbr = {LAFI},
    year = {2023}
}

@inproceedings{lew_lafi_2023,
    author = {Alexander Lew and Eli Sennesh and Jan-Willem {van de Meent} and Vikash Mansinghka},
    booktitle = {POPL Workshop on Languages for Inference (LAFI)},
    title = {{Semantics of Probabilistic Program Traces}},
    html = {https://popl23.sigplan.org/details/lafi-2023-papers/1/Semantics-of-Probabilistic-Program-Traces},
    pdf = {https://popl23.sigplan.org/details?action-call-with-get-request-type=1&173b82200a714176bf311ea4c21818eeaction_17426506610d2b6f0130614628d0da5e92d54a87dc5=1&__ajax_runtime_request__=1&context=POPL-2023&track=lafi-2023-papers&urlKey=1&decoTitle=Semantics-of-Probabilistic-Program-Traces},
    abbr = {LAFI},
    year = {2023}
}

@article{mcinerney2022that,
  title = {That's the {{Wrong Lung}}! {{Evaluating}} and {{Improving}} the {{Interpretability}} of {{Unsupervised Multimodal Encoders}} for {{Medical Data}}},
  author = {McInerney, Denis Jered and Young, Geoffrey and {van de Meent}, Jan-Willem and Wallace, Byron C.},
  year = {2022},
  month = dec,
  journal = {Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing},
  volume = {2022},
  pages = {3626--3648},
  urldate = {2023-09-22},
  abstract = {Pretraining multimodal models on Electronic Health Records (EHRs) provides a means of learning representations that can transfer to downstream tasks with minimal supervision. Recent multimodal models induce soft local alignments between image regions and sentences. This is of particular interest in the medical domain, where alignments might highlight regions in an image relevant to specific phenomena described in free-text. While past work has suggested that attention ``heatmaps'' can be interpreted in this manner, there has been little evaluation of such alignments. We compare alignments from a state-of-the-art multimodal (image and text) model for EHR with human annotations that link image regions to sentences. Our main finding is that the text has an often weak or unintuitive influence on attention; alignments do not consistently reflect basic anatomical information. Moreover, synthetic modifications \textemdash{} such as substituting ``left'' for ``right'' \textemdash{} do not substantially influence highlights. Simple techniques such as allowing the model to opt out of attending to the image and few-shot finetuning show promise in terms of their ability to improve alignments with very little or no supervision. We make our code and checkpoints open-source.},
  pmcid = {PMC10124183},
  pmid = {37103483},
  abbr = {EMNLP}
}

@inproceedings{
esmaeili2022understanding,
title={Understanding Optimization Challenges when Encoding to Geometric Structures},
author={Babak Esmaeili and Robin Walters and Heiko Zimmermann and Jan-Willem van de Meent},
booktitle={NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations},
year={2022},
url={https://openreview.net/forum?id=s3vd1i471Rc},
abbr={NeurIPS WSGNR}
}

@article{smedemark-margulies2022probabilistic,
  title = {Probabilistic Program Inference in Network-Based Epidemiological Simulations},
  author = {{Smedemark-Margulies}, Niklas and Walters, Robin and Zimmermann, Heiko and Laird, Lucas and van der Loo, Christian and Kaushik, Neela and Caceres, Rajmonda and van de Meent, Jan-Willem},
  year = {2022},
  month = nov,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {11},
  pages = {e1010591},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010591},
  urldate = {2023-04-18},
  abstract = {Accurate epidemiological models require parameter estimates that account for mobility patterns and social network structure. We demonstrate the effectiveness of probabilistic programming for parameter inference in these models. We consider an agent-based simulation that represents mobility networks as degree-corrected stochastic block models, whose parameters we estimate from cell phone co-location data. We then use probabilistic program inference methods to approximate the distribution over disease transmission parameters conditioned on reported cases and deaths. Our experiments demonstrate that the resulting models improve the quality of fit in multiple geographies relative to baselines that do not model network topology.},
  langid = {english},
  keywords = {Agent-based modeling,Algorithms,Approximation methods,Graphs,Language,Monte Carlo method,Network analysis,Simulation and modeling},
  html = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010591},
  pdf = {https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1010591&type=printable},
  abbr = {PLOS Comp Bio}
}

@article{sennesh2022interoception,
  title = {Interoception as Modeling, Allostasis as Control},
  author = {Sennesh, Eli and Theriault, Jordan and Brooks, Dana and van de Meent, Jan-Willem and Barrett, Lisa Feldman and Quigley, Karen S.},
  options = {useprefix=true},
  date = {2022-01-01},
  year = {2022},
  journal = {Biological Psychology},
  journaltitle = {Biological Psychology},
  shortjournal = {Biological Psychology},
  volume = {167},
  pages = {108242},
  issn = {0301-0511},
  doi = {10.1016/j.biopsycho.2021.108242},
  html = {https://www.sciencedirect.com/science/article/pii/S0301051121002350},
  urldate = {2022-04-22},
  abstract = {The brain regulates the body by anticipating its needs and attempting to meet them before they arise – a process called allostasis. Allostasis requires a model of the changing sensory conditions within the body, a process called interoception. In this paper, we examine how interoception may provide performance feedback for allostasis. We suggest studying allostasis in terms of control theory, reviewing control theory’s applications to related issues in physiology, motor control, and decision making. We synthesize these by relating them to the important properties of allostatic regulation as a control problem. We then sketch a novel formalism for how the brain might perform allostatic control of the viscera by analogy to skeletomotor control, including a mathematical view on how interoception acts as performance feedback for allostasis. Finally, we suggest ways to test implications of our hypotheses.},
  langid = {english},
  keywords = {Allostasis,Interoception,Predictive processing},
  html = {https://doi.org/10.1016/j.biopsycho.2021.108242},
  pdf = {https://files.osf.io/v1/resources/2ymuj/providers/osfstorage/61c3782b72da2300d7bf9b50?action=download&direct&version=1},
  abbr = {Bio. Pysch.}
}

@article{khan2022computational,
  title = {A {{Computational Neural Model}} for {{Mapping Degenerate Neural Architectures}}},
  author = {Khan, Zulqarnain and Wang, Yiyu and Sennesh, Eli and Dy, Jennifer and Ostadabbas, Sarah and {van de Meent}, Jan-Willem and Hutchinson, J. Benjamin and Satpute, Ajay B.},
  year = {2022},
  month = mar,
  journal = {Neuroinformatics},
  issn = {1559-0089},
  doi = {10.1007/s12021-022-09580-9},
  abstract = {Degeneracy in biological systems refers to a many-to-one mapping between physical structures and their functional (including psychological) outcomes. Despite the ubiquity of the phenomenon, traditional analytical tools for modeling degeneracy in neuroscience are extremely limited. In this study, we generated synthetic datasets to describe three situations of degeneracy in fMRI data to demonstrate the limitations of the current univariate approach. We describe a novel computational approach for the analysis referred to as neural topographic factor analysis (NTFA). NTFA is designed to capture variations in neural activity across task conditions and participants. The advantage of this discovery-oriented approach is to reveal whether and how experimental trials and participants cluster into task conditions and participant groups. We applied NTFA on simulated data, revealing the appropriate degeneracy assumption in all three situations and demonstrating NTFA's utility in uncovering degeneracy. Lastly, we discussed the importance of testing degeneracy in fMRI~data and the implications of applying NTFA to do so.},
  langid = {english},
  abbr = {Neuroinf.},
  html = {https://doi.org/10.1007/s12021-022-09580-9},
  pdf = {https://link.springer.com/content/pdf/10.1007/s12021-022-09580-9.pdf}
}

@InProceedings{Bateni2022_TransductiveCNAPS,
    author    = {Bateni, Peyman and Barber, Jarred and van de Meent, Jan-Willem and Wood, Frank},
    title     = {Enhancing Few-Shot Image Classification With Unlabelled Examples},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2022},
    pages     = {2796-2805},
    abbr      = {WACV},
    abstract  = {We develop a transductive meta-learning method that uses unlabelled instances to improve few-shot image classification performance. Our approach combines a regularized Mahalanobis-distance-based soft k-means clustering procedure with a modified state of the art neural adaptive feature extractor to achieve improved test-time classification accuracy using unlabelled data. We evaluate our method on transductive few-shot learning tasks, in which the goal is to jointly predict labels for query (test) examples given a set of support (training) examples. We achieve state of the art performance on the Meta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. All trained models and code have been made publicly available},
    pdf       = {https://openaccess.thecvf.com/content/WACV2022/papers/Bateni_Enhancing_Few-Shot_Image_Classification_With_Unlabelled_Examples_WACV_2022_paper.pdf},
    html = {https://doi.org/10.1109/WACV51458.2022.00166},
    code = {https://github.com/plai-group/simple-cnaps}
}


@inproceedings{bozkurt2021rateregularization,
  title = {Rate-{{Regularization}} and {{Generalization}} in {{Variational Autoencoders}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Bozkurt, Alican and Esmaeili, Babak and Tristan, Jean-Baptiste and Brooks, Dana and Dy, Jennifer and van de Meent, Jan-Willem},
  year = {2021},
  month = mar,
  pages = {3880--3888},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Variational autoencoders (VAEs) optimize an objective that comprises a reconstruction loss (the distortion) and a KL term (the rate). The rate is an upper bound on the mutual information, which is...},
  langid = {english},
  abbr = {AISTATS},
  html = {https://proceedings.mlr.press/v130/bozkurt21a.html},
  pdf = {http://proceedings.mlr.press/v130/bozkurt21a/bozkurt21a.pdf}
}


@inproceedings{stites-zimmerman2021LearningProposals,
  title       = {Learning proposals for probabilistic programs with inference combinators},
  author      = {Stites, Sam and Zimmermann, Heiko and Wu, Hao and Sennesh, Eli and van de Meent, Jan-Willem},
  shortauthor = {Stites and Zimmermann},
  booktitle   = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages       = {1056--1066},
  year        = {2021},
  editor      = {de Campos, Cassio and Maathuis, Marloes H.},
  volume      = {161},
  series      = {Proceedings of Machine Learning Research},
  month       = {27--30 Jul},
  publisher   = {PMLR},
  pdf         = {https://proceedings.mlr.press/v161/stites21a/stites21a.pdf},
  html        = {https://proceedings.mlr.press/v161/stites21a.html},
  code        = {https://github.com/probtorch/combinators},
  abstract    = {We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing.},
  abbr = {UAI}
}

@inproceedings{zhang2021disentangling,
  title = {Disentangling {{Representations}} of {{Text}} by {{Masking Transformers}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Xiongyi and {van de Meent}, Jan-Willem and Wallace, Byron},
  year = {2021},
  month = nov,
  pages = {778--791},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.60},
  abstract = {Representations from large pretrained models such as BERT encode a range of features into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspects. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, toxicity from dialect in Tweets, and syntax from semantics. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as \textemdash{} and often better than \textemdash previously proposed methods based on variational autoencoders and adversarial training.},
  html = {http://dx.doi.org/10.18653/v1/2021.emnlp-main.60},
  pdf = {https://aclanthology.org/2021.emnlp-main.60.pdf},
  abbr = {EMNLP}
}

@inproceedings{amir2021impact,
  title = {On the {{Impact}} of {{Random Seeds}} on the {{Fairness}} of {{Clinical Classifiers}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Amir, Silvio and {van de Meent}, Jan-Willem and Wallace, Byron},
  year = {2021},
  month = jun,
  pages = {3808--3823},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.299},
  abstract = {Recent work has shown that fine-tuning large networks is surprisingly sensitive to changes in random seed(s). We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III \textemdash\textemdash{} the standard dataset in clinical NLP research. Apparent subgroup performance varies substantially for seeds that yield similar overall performance, although there is no evidence of a trade-off between overall and subgroup performance. However, we also find that the small sample sizes inherent to looking at intersections of minority groups and somewhat rare conditions limit our ability to accurately estimate disparities. Further, we find that jointly optimizing for high overall performance and low disparities does not yield statistically significant improvements. Our results suggest that fairness work using MIMIC-III should carefully account for variations in apparent differences that may arise from stochasticity and small sample sizes.},
  abbr = {NAACL},
  html = {http://dx.doi.org/10.18653/v1/2021.naacl-main.299},
  pdf = {https://aclanthology.org/2021.naacl-main.299.pdf}
}

@inproceedings{zimmermann2021nested,
  title={Nested Variational Inference},
  author={Heiko Zimmermann and Hao Wu and Babak Esmaeili and Jan-Willem van de Meent},
  booktitle = {Advances in Neural Information Processing Systems},
  year={2021},
  volume={34},
  html={https://proceedings.neurips.cc/paper/2021/hash/ab49b208848abe14418090d95df0d590-Abstract.html},
  pdf={https://proceedings.neurips.cc/paper/2021/file/ab49b208848abe14418090d95df0d590-Paper.pdf},
  abstract={We develop nested variational inference (NVI), a family of methods that learn proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristics to guide the sampler. Our experiments apply NVI to (a) sample from a multimodal distribution using a learned annealing path (b) learn heuristics that approximate the likelihood of future observations in a hidden Markov model and (c) to perform amortized inference in hierarchical deep generative models. We observe that optimizing nested objectives leads to improved sample quality in terms of log average weight and effective sample size.},
  abbr = {NeurIPS}
}

@inproceedings{biza2021action,
  title = {Action {{Priors}} for {{Large Action Spaces}} in {{Robotics}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  author = {Biza, Ondrej and Wang, Dian and Platt, Robert and van de Meent, Jan-Willem and Wong, Lawson L.S.},
  options = {useprefix=true},
  year = {2021},
  date = {2021-05-03},
  series = {{{AAMAS}} '21},
  pages = {205--213},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  location = {{Richland, SC}},
  abstract = {In robotics, it is often not possible to learn useful policies using pure model-free reinforcement learning without significant reward shaping or curriculum learning. As a consequence, many researchers rely on expert demonstrations to guide learning. However, acquiring expert demonstrations can be expensive. This paper proposes an alternative approach where the solutions of previously solved tasks are used to produce an action prior that can facilitate exploration in future tasks. The action prior is a probability distribution over actions that summarizes the set of policies found solving previous tasks. Our results indicate that this approach can be used to solve robotic manipulation problems that would otherwise be infeasible without expert demonstrations. Source code is available at https://github.com/ondrejba/action\_priors.},
  isbn = {978-1-4503-8307-3},
  keywords = {action prior,deep learning,reinforcement learning,robotic manipulation,robotics},
  code = {https://github.com/ondrejba/action\_priors},
  pdf = {https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p205.pdf},
  html = {https://dl.acm.org/doi/10.5555/3463952.3463982},
  abbr = {AAMAS}
}
@inproceedings{wu2021conjugate,
  title = {Conjugate Energy-Based Models},
  abstract = {In this paper, we propose conjugate energy-based models (CEBMs), a new class of energy-based models that define a joint density over data and latent variables. The joint density of a CEBM decomposes into an intractable distribution over data and a tractable posterior over latent variables. CEBMs have similar use cases as variational autoencoders, in the sense that they learn an unsupervised mapping from data to latent variables. However, these models omit a generator network, which allows them to learn more flexible notions of similarity between data points. Our experiments demonstrate that conjugate EBMs achieve competitive results in terms of image modelling, predictive power of latent space, and out-of-domain detection on a variety of datasets.},
  author = {Wu*, Hao and Esmaeili*, Babak and Wick, Michael L and Tristan, Jean-Baptiste and {van de Meent}, Jan-Willem},
  booktitle = {International Conference on Machine Learning},
  volume = {139},
  pages = {11228--11239},
  year = {2021},
  series = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
  abbr = {ICML}
}

@article{sennesh2020neural,
  title = {Neural {{Topographic Factor Analysis}} for {{fMRI Data}}},
  author = {Sennesh*, Eli and Khan*, Zulqarnain and Wang, Yiyu and Hutchinson, J. Benjamin and Satpute, Ajay and Dy, Jennifer and {van de Meent}, Jan-Willem},
  year = {2020},
  volume = {33},
  journal = {Advances in Neural Information Processing Systems},
  language = {en},
  abbr = {NeurIPS},
  html = {https://proceedings.neurips.cc/paper/2020/hash/8c3c27ac7d298331a1bdfd0a5e8703d3-Abstract.html},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/8c3c27ac7d298331a1bdfd0a5e8703d3-Paper.pdf},
  code = {https://github.com/neu-pml/HTFATorch}
}

@incollection{wu2020amortized,
 abstract = {Amortized variational methods have proven difficult to scale to structured problems, such as inferring positions of multiple objects from video images. We develop amortized population Gibbs (APG) samplers, a class of scalable methods that frame structured variational inference as adaptive importance sampling. APG samplers construct high-dimensional proposals by iterating over updates to lower-dimensional blocks of variables. We train each conditional proposal by minimizing the inclusive KL divergence with respect to the conditional posterior. To appropriately account for the size of the input data, we develop a new parameterization in terms of neural sufficient statistics. Experiments show that APG samplers can be used to train highly-structured deep generative models in an unsupervised manner, and achieve substantial improvements in inference accuracy relative to standard autoencoding variational methods.},
 author = {Wu, Hao and Zimmermann, Heiko and Sennesh, Eli and Le, Tuan Anh and {van de Meent}, Jan-Willem},
 booktitle = {Proceedings of the International Conference on Machine Learning},
 pages = {10205--10215},
 title = {Amortized Population Gibbs Samplers with Neural Sufficient Statistics},
 year = {2020},
 html = {https://proceedings.mlr.press/v119/wu20h.html},
 pdf = {http://proceedings.mlr.press/v119/wu20h/wu20h.pdf},
 abbr = {ICML}
}

@article{mcinerney2020query-focused,
  title = {Query-{{Focused EHR Summarization}} to {{Aid Imaging Diagnosis}}},
  author = {McInerney, Denis Jered and Dabiri, Borna and Touret, Anne-Sophie and Young, Geoffrey and {van de Meent}, Jan-Willem and Wallace, Byron C.},
  year = {2020},
  month = apr,
  abstract = {Electronic Health Records (EHRs) provide vital contextual information to radiologists and other physicians when making a diagnosis. Unfortunately, because a given patient's record may contain hundreds of notes and reports, identifying relevant information within these in the short time typically allotted to a case is very difficult. We propose and evaluate models that extract relevant text snippets from patient records to provide a rough case summary intended to aid physicians considering one or more diagnoses. This is hard because direct supervision (i.e., physician annotations of snippets relevant to specific diagnoses in medical records) is prohibitively expensive to collect at scale. We propose a distantly supervised strategy in which we use groups of International Classification of Diseases (ICD) codes observed in 'future' records as noisy proxies for 'downstream' diagnoses. Using this we train a transformer-based neural model to perform extractive summarization conditioned on potential diagnoses. This model defines an attention mechanism that is conditioned on potential diagnoses (queries) provided by the diagnosing physician. We train (via distant supervision) and evaluate variants of this model on EHR data from a local hospital and MIMIC-III (the latter to facilitate reproducibility). Evaluations performed by radiologists demonstrate that these distantly supervised models yield better extractive summaries than do unsupervised approaches. Such models may aid diagnosis by identifying sentences in past patient reports that are clinically relevant to a potential diagnoses.},
  archivePrefix = {arXiv},
  eprint = {2004.04645},
  eprinttype = {arxiv},
  journal = {Machine Learning for Healthcare},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat},
  html = {https://proceedings.mlr.press/v126/mcinerney20a.html},
  pdf = {http://proceedings.mlr.press/v126/mcinerney20a/mcinerney20a.pdf},
  abbr = {MLHC}
}

@article{esmaeili2018structured,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.02086},
  primaryClass = {cs, stat},
  title = {Structured {{Disentangled Representations}}},
  abstract = {Deep latent-variable models learn representations of high-dimensional data in an unsupervised manner. A number of recent efforts have focused on learning representations that disentangle statistically independent axes of variation by introducing modifications to the standard objective function. These approaches generally assume a simple diagonal Gaussian prior and as a result are not able to reliably disentangle discrete factors of variation. We propose a two-level hierarchical objective to control relative degree of statistical independence between blocks of variables and individual variables within blocks. We derive this objective as a generalization of the evidence lower bound, which allows us to explicitly represent the trade-offs between mutual information between data and representation, KL divergence between representation and prior, and coverage of the support of the empirical data distribution. Experiments on a variety of datasets demonstrate that our objective can not only disentangle discrete variables, but that doing so also improves disentanglement of other variables and, importantly, generalization even to unseen combinations of factors.},
  journal = {Artificial Intelligence and Statistics},
  author = {Esmaeili, Babak and Wu, Hao and Jain, Sarthak and Bozkurt, Alican and Siddharth, N. and Paige, Brooks and Brooks, Dana H. and Dy, Jennifer and {van de Meent}, Jan-Willem},
  month = apr,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
}

@article{esmaeili2018structuredb,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.05035},
  primaryClass = {cs},
  title = {Structured {{Neural Topic Models}} for {{Reviews}}},
  abstract = {We present Variational Aspect-based Latent Topic Allocation (VALTA), a family of autoencoding topic models that learn aspect-based representations of reviews. VALTA defines a user-item encoder that maps bag-of-words vectors for combined reviews associated with each paired user and item onto structured embeddings, which in turn define per-aspect topic weights. We model individual reviews in a structured manner by inferring an aspect assignment for each sentence in a given review, where the per-aspect topic weights obtained by the user-item encoder serve to define a mixture over topics, conditioned on the aspect. The result is an autoencoding neural topic model for reviews, which can be trained in a fully unsupervised manner to learn topics that are structured into aspects. Experimental evaluation on large number of datasets demonstrates that aspects are interpretable, yield higher coherence scores than non-structured autoencoding topic model variants, and can be utilized to perform aspect-based comparison and genre discovery.},
  journal = {Artificial Intelligence and Statistics},
  author = {Esmaeili, Babak and Huang, Hongyi and Wallace, Byron C. and {van de Meent}, Jan-Willem},
  month = dec,
  year = {2019},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
}

@inproceedings{jain_emnlp_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.07212},
  title = {Learning {{Disentangled Representations}} of {{Texts}} with {{Application}} to {{Biomedical Abstracts}}},
  abstract = {We propose a method for learning disentangled representations of texts that code for distinct and complementary aspects, with the aim of affording efficient model transfer and interpretability. To induce disentangled embeddings, we propose an adversarial objective based on the (dis)similarity between triplets of documents with respect to specific aspects. Our motivating application is embedding biomedical abstracts describing clinical trials in a manner that disentangles the populations, interventions, and outcomes in a given trial. We show that our method learns representations that encode these clinically salient aspects, and that these can be effectively used to perform aspect-specific retrieval. We demonstrate that the approach generalizes beyond our motivating application in experiments on two multi-aspect review corpora.},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Jain, Sarthak and Banner, Edward and {van de Meent}, Jan-Willem and Marshall, Iain J. and Wallace, Byron C.},
  year = {2018},
}

@inproceedings{siddharth_nips_2017,
    title = {Learning Disentangled Representations with Semi-Supervised Deep Generative Models},
    abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
    author = {Siddharth, N. and Paige, Brooks and van de Meent, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood,
Frank and Torr, Philip},
    booktitle = {Advances in Neural Information Processing Systems 30},
    editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {5927--5937},
    year = {2017},
}

@inproceedings{tolpin_ifl_2016,
 author = {Tolpin, David and van de Meent, Jan-Willem and Yang, Hongseok and Wood, Frank},
 abstract = {Anglican is a probabilistic programming system designed to interoperate with Clojure and other JVM languages. We introduce the programming language Anglican, outline our design choices, and discuss in depth the implementation of the Anglican language and runtime, including macro-based compilation, extended CPS-based evaluation model, and functional representations for probabilistic paradigms, such as a distribution, a random process, and an inference algorithm. We show that a probabilistic functional language can be implemented efficiently and integrated tightly with a conventional functional language with only moderate computational overhead. We also demonstrate how advanced probabilistic modelling concepts are mapped naturally to the functional foundation.},
 title = {Design and Implementation of Probabilistic Programming Language Anglican},
 booktitle = {Proceedings of the 28th Symposium on the Implementation and Application of Functional Programming Languages},
 series = {IFL 2016},
 year = {2016},
 isbn = {978-1-4503-4767-9},
 location = {Leuven, Belgium},
 pages = {6:1--6:12},
 articleno = {6},
 numpages = {12},
 html = {http://doi.acm.org/10.1145/3064899.3064910},
 doi = {10.1145/3064899.3064910},
 acmid = {3064910},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{rainforth_nips_2016,
  title = {Bayesian {O}ptimization for {P}robabilistic {P}rograms},
  author = {Rainforth, Tom and Le, Tuan Anh and van de Meent, Jan-Willem and Osborne, Michael A and Wood, Frank},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {280--288},
  year = {2016},
  annote = {https://github.com/probprog/bopp}
}

@inproceedings{rainforth_icml_2016,
abstract = {We introduce interacting particle Markov chain Monte Carlo (iPMCMC), a PMCMC method that introduces a coupling between multiple standard and conditional sequential Monte Carlo samplers. Like related methods, iPMCMC is a Markov chain Monte Carlo sampler on an extended space. We present empirical results that show significant improvements in mixing rates relative to both non- interacting PMCMC samplers and a single PMCMC sampler with an equivalent total computational budget. An additional advantage of the iPMCMC method is that it is suitable for distributed and multi-core architectures.},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning,},
pages = {2616–2625},
title = {{Interacting Particle Markov Chain Monte Carlo}},
author = {Rainforth, Tom and Naesseth, Christian A. and Lindsten,  Fredrik and Paige, Brooks and van de Meent, Jan-Willem and Doucet, Arnaud and Wood, Frank},
year = {2016}}

@article{vandemeent_aistats_2016,
abstract = {In this work, we explore how probabilistic programs can be used to represent policies in sequential decision problems. In this formulation, a probabilistic program is a black-box stochastic simulator for both the problem domain and the agent. We relate classic policy gradient techniques to recently introduced black-box variational methods which generalize to probabilistic program inference. We present case studies in the Canadian traveler problem, Rock Sample, and a benchmark for optimal diagnosis inspired by Guess Who. Each study illustrates how programs can efficiently represent policies using moderate numbers of parameters.},
journal = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
author = {van de Meent, Jan-Willem and Paige, Brooks and Tolpin, David and Wood, Frank},
pages = {1195–1204},
title = {{Black-Box Policy Search with Probabilistic Programs}},
year = {2016}
}

@incollection{tolpin_ecml_2015,
  year={2015},
  isbn={978-3-319-23524-0},
  booktitle={Machine Learning and Knowledge Discovery in Databases},
  volume={9285},
  series={Lecture Notes in Computer Science},
  editor={Appice, Annalisa and Rodrigues, Pedro Pereira and Santos Costa, Vítor and Gama, João and Jorge, Alípio and Soares, Carlos},
  doi={10.1007/978-3-319-23525-7_19},
  title={Output-Sensitive Adaptive Metropolis-Hastings for Probabilistic Programs},
  url={http://dx.doi.org/10.1007/978-3-319-23525-7_19},
  publisher={Springer International Publishing},
  keywords={Probabilistic programming; Adaptive MCMC},
  author={Tolpin, David and van de Meent, Jan-Willem and Paige, Brooks and Wood, Frank},
  pages={311-326},
  language={English}
}

@inproceedings{vandemeent_aistats_2015,
abstract = {Particle Markov chain Monte Carlo techniques rank among current state-of-the-art methods for probabilistic program inference. A drawback of these techniques is that they rely on importance resampling, which results in degenerate particle trajectories and a low effective sample size for variables sampled early in a program. We here develop a formalism to adapt ancestor resampling, a technique that mitigates particle degeneracy, to the probabilistic programming setting. We present empirical results that demonstrate nontrivial performance gains.},
archivePrefix = {arXiv},
arxivId = {1501.06769},
author = {van de Meent, Jan-Willem and Yang, Hongseok and Mansinghka, Vikash and Wood, Frank},
booktitle = {Artificial Intelligence and Statistics},
eprint = {1501.06769},
title = {{Particle Gibbs with Ancestor Sampling for Probabilistic Programs}},
year = {2015}
}

@inproceedings{wood_aistats_2014,
author = {Wood, Frank and van de Meent, Jan-Willem and Mansinghka, Vikash},
booktitle = {Artificial Intelligence and Statistics},
pages = {1024--1032},
title = {{A new approach to probabilistic programming inference}},
year = {2014}
}

@article{vandemeent_icml_2013,
abstract = {We address the problem of analyzing sets of noisy time-varying signals that all report on the same process but confound straightforward analyses due to complex inter-signal heterogeneities and measurement artifacts. In particular we consider single-molecule experiments which indirectly measure the distinct steps in a biomolecular process via observations of noisy time-dependent signals such as a fluorescence intensity or bead position. Straightforward hidden Markov model (HMM) analyses attempt to characterize such processes in terms of a set of conformational states, the transitions that can occur between these states, and the associated rates at which those transitions occur; but require ad-hoc post-processing steps to combine multiple signals. Here we develop a hierarchically coupled HMM that allows experimentalists to deal with inter-signal variability in a principled and automatic way. Our approach is a generalized expectation maximization hyperparameter point estimation procedure with variational Bayes at the level of individual time series that learns an single interpretable representation of the overall data generating process.},
archivePrefix = {arXiv},
arxivId = {1305.3640},
author = {van de Meent, Jan-Willem and Bronson, Jonathan E and Wood, Frank and Gonzalez, Ruben L. and Wiggins, Chris H.},
eprint = {1305.3640},
journal = {Proceedings of the 30th International Conference on Machine Learning},
month = may,
number = {2},
pages = {361--369},
title = {{Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data}},
volume = {28},
year = {2013}
}

@article{emmett_bpj_2015,
author = {Emmett, Kevin J. and Rosenstein, Jacob K. and van de Meent, Jan-Willem and Shepard, Ken L. and Wiggins, Chris H.},
abstract = {Nanopore sequencing promises long read-lengths and single-molecule resolution, but the stochastic motion of the DNA molecule inside the pore is a current barrier to high accuracy reads. We develop a method of statistical inference that explicitly accounts for this error and demonstrate that high accuracy (>99.9%) sequence inference is feasible even under highly diffusive motion by using a hidden Markov model to jointly analyze multiple stochastic reads. Using this model, we place bounds on achievable inference accuracy under a range of experimental parameters.},
journal = {Biophysical Journal},
volume = {108},
issue = {April},
pages = {1852-1855},
doi = {doi:10.1016/j.bpj.2015.03.013},
title = {{Statistical Inference for Nanopore Sequencing with a Biased Random Walk Model}},
year = {2015}
}

@article{johnson_nar_2014,
abstract = {The bacterial transcription factor LacI loops DNA by binding to two separate locations on the DNA simultaneously. Despite being one of the best-studied model systems for transcriptional regulation, the number and conformations of loop structures accessible to LacI remain unclear, though the importance of multiple coexisting loops has been implicated in interactions between LacI and other cellular regulators of gene expression. To probe this issue, we have developed a new analysis method for tethered particle motion, a versatile and commonly used in vitro single-molecule technique. Our method, vbTPM, performs variational Bayesian inference in hidden Markov models. It learns the number of distinct states (i.e. DNA-protein conformations) directly from tethered particle motion data with better resolution than existing methods, while easily correcting for common experimental artifacts. Studying short (roughly 100 bp) LacI-mediated loops, we provide evidence for three distinct loop structures, more than previously reported in single-molecule studies. Moreover, our results confirm that changes in LacI conformation and DNA-binding topology both contribute to the repertoire of LacI-mediated loops formed in vitro, and provide qualitatively new input for models of looping and transcriptional regulation. We expect vbTPM to be broadly useful for probing complex protein-nucleic acid interactions.},
archivePrefix = {arXiv},
arxivId = {1402.0894},
author = {Johnson, S. and van de Meent, J.-W. and Phillips, R. and Wiggins, C. H. and Linden, M.},
doi = {10.1093/nar/gku563},
eprint = {1402.0894},
issn = {0305-1048},
journal = {Nucleic Acids Research},
pages = {gku563--},
title = {{Multiple LacI-mediated loops revealed by Bayesian statistics and tethered particle motion}},
year = {2014}
}

@article{gansell_jas_2014,
abstract = {Thousands of first-millennium BCE ivory carvings have been excavated from Neo-Assyrian sites in Mesopotamia (primarily Nimrud, Khorsabad, and Arslan Tash), hundreds of miles from their Levantine production contexts. At present, their specific manufacture dates and workshop localities are unknown. Relying on subjective, visual methods, scholars have grappled with their classification and regional attribution for over a century. This study combines visual approaches with machine learning techniques to offer data-driven perspectives on the classification and attribution of this Iron Age corpus.The study sample consists of 162 sculptures of female figures that have been conventionally attributed to three main regional carving traditions: "Phoenician," "North Syrian," and "Syrian/South Syrian". We have developed an algorithm that clusters the ivories based on a combination of descriptive and anthropometric data. The resulting categories, which are based on purely statistical criteria, show good agreement with conventional art historical classifications, while revealing new insights, especially with regard to the "Syrian/South Syrian" tradition.Specifically, we have determined that objects of the Syrian/South Syrian tradition might be more closely related to Phoenician objects than to North Syrian objects. We also reconsider the classification of a subset of "Phoenician" objects, and we confirm Syrian/South Syrian stylistic subgroups, the geographic distribution of which might illuminate Neo-Assyrian acquisition networks. Additionally, we have identified the features in our cluster assignments that might be diagnostic of regional traditions. In short, our study both corroborates traditional visual classifications and demonstrates how machine learning techniques may be employed to retrieve complementary information not accessible through an exclusively visual analysis. ?? 2013 Elsevier Ltd.},
archivePrefix = {arXiv},
arxivId = {1401.0871},
author = {Gansell, Amy Rebecca and van de Meent, Jan Willem and Zairis, Sakellarios and Wiggins, Chris H.},
doi = {10.1016/j.jas.2013.11.005},
eprint = {1401.0871},
issn = {10959238},
journal = {Journal of Archaeological Science},
keywords = {Attribution,Clustering,Iron Age,Ivory sculpture,Levant,Machine learning,Mutual information},
month = apr,
pages = {194--205},
publisher = {Elsevier Ltd},
title = {{Stylistic clusters and the Syrian/South Syrian tradition of first-millennium BCE Levantine ivory carving: A machine learning approach}},
volume = {44},
year = {2014}
}

@article{vandemeent_bpj_2014,
abstract = {Many single-molecule experiments aim to characterize biomolecular processes in terms of kinetic models that specify the rates of transition between conformational states of the biomolecule. Estimation of these rates often requires analysis of a population of molecules, in which the conformational trajectory of each molecule is represented by a noisy, time-dependent signal trajectory. Although hidden Markov models (HMMs) may be used to infer the conformational trajectories of individual molecules, estimating a consensus kinetic model from the population of inferred conformational trajectories remains a statistically difficult task, as inferred parameters vary widely within a population. Here, we demonstrate how a recently developed empirical Bayesian method for HMMs can be extended to enable a more automated and statistically principled approach to two widely occurring tasks in the analysis of single-molecule fluorescence resonance energy transfer (smFRET) experiments: 1), the characterization of changes in rates across a series of experiments performed under variable conditions; and 2), the detection of degenerate states that exhibit the same FRET efficiency but differ in their rates of transition. We apply this newly developed methodology to two studies of the bacterial ribosome, each exemplary of one of these two analysis tasks. We conclude with a discussion of model-selection techniques for determination of the appropriate number of conformational states. The code used to perform this analysis and a basic graphical user interface front end are available as open source software.},
author = {van de Meent, Jan-Willem and Bronson, Jonathan E and Wiggins, Chris H and Gonzalez, Ruben L},
doi = {10.1016/j.bpj.2013.12.055},
issn = {1542-0086},
journal = {Biophysical journal},
month = mar,
number = {6},
pages = {1327--37},
pmid = {24655508},
title = {{Empirical Bayes methods enable advanced population-level analyses of single-molecule FRET experiments.}},
volume = {106},
year = {2014}
}

@article{vandemeent_jfm_2010,
author = {van de Meent, Jan-Willem and Sederman, Andy J. and Gladden, Lynn F. and Goldstein, Raymond E.},
doi = {10.1017/S0022112009992187},
issn = {0022-1120},
journal = {Journal of Fluid Mechanics},
pages = {5--14},
title = {{Measurement of cytoplasmic streaming in single plant cells by magnetic resonance velocimetry}},
volume = {642},
year = {2010}
}

@article{sultan_epl_2010,
author = {Sultan, Eric and van de Meent, Jan-Willem and Somfai, Ellak and Morozov, Alexander N. and van Saarloos, Wim},
doi = {10.1209/0295-5075/90/64002},
issn = {0295-5075},
journal = {Europhysics Letters},
month = jun,
number = {6},
pages = {64002},
title = {{Polymer rheology simulations at the meso- and macroscopic scale}},
volume = {90},
year = {2010}
}

@article{vandemeent_prl_2008,
author = {van de Meent, Jan-Willem and Tuval, Idan and Goldstein, Raymond},
doi = {10.1103/PhysRevLett.101.178102},
issn = {0031-9007},
journal = {Physical Review Letters},
month = oct,
number = {17},
pages = {178102},
title = {{Nature’s Microfluidic Transporter: Rotational Cytoplasmic Streaming at High P\'{e}clet Numbers}},
volume = {101},
year = {2008}
}

@article{goldstein_pnas_2008,
abstract = {Found in many large eukaryotic cells, particularly in plants, cytoplasmic streaming is the circulation of their contents driven by fluid entrainment from particles carried by molecular motors at the cell periphery. In the more than two centuries since its discovery, streaming has frequently been conjectured to aid in transport and mixing of molecular species in the cytoplasm and, by implication, in cellular homeostasis, yet no theoretical analysis has been presented to quantify these processes. We show by a solution to the coupled dynamics of fluid flow and diffusion appropriate to the archetypal "rotational streaming" of algal species such as Chara and Nitella that internal mixing and the transient dynamical response to changing external conditions can indeed be enhanced by streaming, but to an extent that depends strongly on the pitch of the helical flow. The possibility that this may have a developmental consequence is illustrated by the coincidence of the exponential growth phase of Nitella and the point of maximum enhancement of those processes.},
author = {Goldstein, Raymond E and Tuval, Idan and van de Meent, Jan-Willem},
doi = {10.1073/pnas.0707223105},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = mar,
number = {10},
pages = {3663--7},
pmid = {18310326},
title = {{Microfluidics of cytoplasmic streaming and its implications for intracellular transport.}},
volume = {105},
year = {2008}
}

@article{vandemeent_pre_2008,
author = {van de Meent, Jan-Willem and Morozov, Alexander and Somfai, Ell\'{a}k and Sultan, Eric and van Saarloos, Wim},
doi = {10.1103/PhysRevE.78.015701},
issn = {1539-3755},
journal = {Physical Review E},
month = jul,
number = {1},
pages = {015701},
title = {{Coherent structures in dissipative particle dynamics simulations of the transition to turbulence in compressible shear flows}},
volume = {78},
year = {2008}
}

@article{fenistein_prl_2006,
author = {Fenistein, Denis and van de Meent, Jan-Willem and van Hecke, Martin},
doi = {10.1103/PhysRevLett.96.118001},
issn = {0031-9007},
journal = {Physical Review Letters},
month = mar,
number = {11},
pages = {118001},
title = {{Core Precession and Global Modes in Granular Bulk Flow}},
volume = {96},
year = {2006}
}

@article{fenistein_prl_2004,
author = {Fenistein, Denis and van de Meent, Jan and van Hecke, Martin},
doi = {10.1103/PhysRevLett.92.094301},
issn = {0031-9007},
journal = {Physical Review Letters},
month = mar,
number = {9},
pages = {094301},
title = {{Universal and Wide Shear Zones in Granular Bulk Flow}},
volume = {92},
year = {2004}
}
