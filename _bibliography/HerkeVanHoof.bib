@inproceedings{macfarlane2026gradient,
  title={Gradient-Based Program Synthesis with Neurally Interpreted Languages},
  author={Macfarlane, Matthew and Bonnet, Cl\'{e}ment and van Hoof, Herke and Lelis, Levi},
  booktitle={International Conference on Representation Learning},
  year={2026},
  abbr={ICLR},
  html={https://openreview.net/forum?id=NAORIWBaoO},
  pdf={https://openreview.net/pdf?id=NAORIWBaoO},
}

@inproceedings{pol2022multi,
title = {Multi-Agent MDP Homomorphic Networks},
author = {van der Pol, Elise and van Hoof, Herke and Oliehoek, Frans and Welling, Max},
year = {2022},
date = {25-04-2022},
booktitle = {International Conference on Learning Representations},
keywords = {},
pubstate = {forthcoming},
abbr={ICLR},
abstract={This paper introduces Multi-Agent MDP Homomorphic Networks, a class of networks that allows distributed execution using only local information, yet is able to share experience between global symmetries in the joint state-action space of cooperative multi-agent systems. In cooperative multi-agent systems, complex symmetries arise between different configurations of the agents and their local observations. For example, consider a group of agents navigating: rotating the state globally results in a permutation of the optimal joint policy. Existing work on symmetries in single agent reinforcement learning can only be generalized to the fully centralized setting, because such approaches rely on the global symmetry in the full state-action spaces, and these can result in correspondences across agents. To encode such symmetries while still allowing distributed execution we propose a factorization that decomposes global symmetries into local transformations. Our proposed factorization allows for distributing the computation that enforces global symmetries over local agents and local interactions. We introduce a multi-agent equivariant policy network based on this factorization. We show empirically on symmetric multi-agent problems that globally symmetric distributable policies improve data efficiency compared to non-equivariant baselines.},
selected=false,
html={https://openreview.net/forum?id=H7HDG\hyphen\hyphenDJF0},
pdf={https://openreview.net/pdf?id=H7HDG\hyphen\hyphenDJF0}
}

@inproceedings{kool2020estimating,
title = {Estimating Gradients for Discrete Random Variables by Sampling without Replacement},
author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
url = {https://openreview.net/pdf?id=rklEj2EFvB
https://youtu.be/KtP-Z2bvPPE},
year = {2020},
date = {2020-04-26},
booktitle = {International Conference on Learning Representations},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings},
abbr={ICLR},
abstract={We derive an unbiased estimator for expectations over discrete random variables based on sampling without replacement, which reduces variance as it avoids duplicate samples. We show that our estimator can be derived as the Rao-Blackwellization of three different estimators. Combining our estimator with REINFORCE, we obtain a policy gradient estimator and we reduce its variance using a built-in control variate which is obtained without additional model evaluations. The resulting estimator is closely related to other gradient estimators. Experiments with a toy problem, a categorical Variational Auto-Encoder and a structured prediction problem show that our estimator is the only estimator that is consistently among the best estimators in both high and low entropy settings.},
video={https://youtu.be/KtP-Z2bvPPE},
code={https://github.com/wouterkool/estimating-gradients-without-replacement},
selected=true,
html={https://openreview.net/forum?id=rklEj2EFvB},
pdf={https://openreview.net/pdf?id=rklEj2EFvB},
}


@inproceedings{fujimoto2018addressing,
title = {Addressing function approximation error in actor-critic methods},
author = {Fujimoto, S. and van Hoof, H. and Meger, D. },
url = {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf
http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a-supp.pdf},
year = {2018},
date = {2018-07-10},
booktitle = {International Conference on Machine Learning},
pages = {1587--1596},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings},
html={http://proceedings.mlr.press/v80/fujimoto18a.pdf},
pdf={http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
abbr={ICML}
}

@inproceedings{smith2018inference,
title = {An Inference-Based Policy Gradient Method for Learning Options},
author = {Smith, M. and van Hoof, H. and Pineau, J.},
url = {http://proceedings.mlr.press/v80/smith18a/smith18a.pdf},
year = {2018},
date = {2018-07-10},
booktitle = {International Conference on Machine Learning},
pages = {4703-4712},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings},
html={https://proceedings.mlr.press/v80/smith18a.html},
pdf={ttp://proceedings.mlr.press/v80/smith18a/smith18a.pdf},
abbr={ICML}
}

@article{hoof2017nonparametric,
title = {Non-parametric Policy Search with Limited Information Loss},
author = {Van Hoof, H. and Neumann, G. and Peters, J.},
editor = {Murphy, K.},
url = {http://jmlr.org/papers/volume18/16-142/16-142.pdf},
year = {2017},
date = {2017-08-01},
journal = {Journal of Machine Learning Research},
volume = {18},
number = {73},
pages = {1-46},
keywords = {},
pubstate = {published},
tppubtype = {article},
html={https://jmlr.org/papers/v18/16-142.html},
pdf={http://jmlr.org/papers/volume18/16-142/16-142.pdf},
abbr={JMLR}
}

@inproceedings{wohlke2022value,
title = {Value Refinement Network (VRN)},
author = {Jan Wöhlke and Felix Schmitt and Herke van Hoof},
url = {https://openreview.net/forum?id=iUt2KYdXBDD},
year = {2022},
date = {23-7-2022},
booktitle = {International Joint Conference on Artificial Intelligence},
keywords = {},
pubstate = {forthcoming},
tppubtype = {inproceedings},
html={https://openreview.net/forum?id=iUt2KYdXBDD},
abbr={IJCAI}
}

@inproceedings{hopner2022leveraging,
title = {Leveraging class abstraction for commonsense reinforcement learning via residual policy gradient methods},
author = {Höpner, Niklas and Tiddi, Ilaria and van Hoof, Herke},
url = {https://arxiv.org/abs/2201.12126},
year = {2022},
date = {23-7-2022},
booktitle = {International Joint Conference on Artificial Intelligence},
keywords = {},
pubstate = {forthcoming},
tppubtype = {inproceedings},
html={https://arxiv.org/abs/2201.12126},
pdf={https://arxiv.org/pdf/2201.12126.pdf},
abbr={IJCAI}
}

@inproceedings{kuric2024planning,
title = {Planning with a Learned Policy Basis to Optimally Solve Complex Tasks},
author = {Kuric, D. and Infante, G. and Gómez, V. and Jonsson, A. and van Hoof, H. },
url = {https://openreview.net/forum?id=6N1uCtBhcL},
year = {2024},
date = {2024-06-01},
booktitle = {International Conference on Automated Planning and Scheduling},
keywords = {},
pubstate = {forthcoming},
tppubtype = {inproceedings},
html={https://openreview.net/forum?id=6N1uCtBhcL},
pdf={https://openreview.net/pdf?id=6N1uCtBhcL},
abbr={ICAPS}
}

@article{kuric2023reusable,
title = {Reusable Options through Gradient-based Meta Learning},
author = {David Kuric and Herke Hoof},
html = {https://openreview.net/forum?id=qdDmxzGuzu},
pdf={https://openreview.net/pdf?id=qdDmxzGuzu},
year = {2023},
date = {2023-03-28},
urldate = {2023-03-28},
journal = {Transactions on Machine Learning Research},
volume = {03/2023},
keywords = {},
pubstate = {published},
tppubtype = {article},
abbr={TMLR}
}
