<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>AMLab  | Amsterdam Machine Learning Lab | Tandersonkeller</title>
    <meta name="author" content="AMLab  | Amsterdam Machine Learning Lab" />
    <meta name="description" content="My first name is Thomas, but most people call me by my middle name – Andy. I’m currently a third-year PhD student supervised by Max Welling at the University of Amsterdam. My work is focused on unsupervised structured representation learning, inspired and supported by observations from neuroscience. In pursuit of this goal, during my PhD I have developed novel methods for probabilistic generative modeling which make use of biologically plausible mechanisms such as learned feedback connections and topographic organization to approximate otherwise analytically intractible solutions. In the long term, I hope to be able to answer the question of how transformations and invariances are learned and encoded in the brain, and further understand how the 2-dimensional structure of the cortical surface shapes how learning proceeds. More immediately, my current interests broadly include: developing unsupervised methods for learning approximately equivariant & invariant representations, exploring the computational benefits of topographically organized representations, and improving techniques for efficiently training deep latent variable models.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/amlab-icon.png"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://amlab-amsterdam.github.io/people/TAndersonKeller/">
    
    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://AMLab-Amsterdam.github.io/"><span class="font-weight-bold">AMLab</span>   | Amsterdam Machine Learning Lab</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li> -->
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/contact/">Contact</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/joining/">Joining</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/people/">People</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- person-page.html -->
<div class="post">

<!--   <header class="post-header">
    <h1 class="post-title"></h1>
    <p class="post-description">Biologically-inspired Unsupervised Structured Representation Learning</p>
  </header>
 -->
  <article>
    <div class="clearfix">
<div class="person-header">
<div class="person-headshot float-left">
    <figure>

  <picture>
<!--     <source media="(max-width: 480px)" srcset="/assets/img/TAndersonKeller-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/TAndersonKeller-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/TAndersonKeller-1400.webp" />
    -->
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/TAndersonKeller.jpg" title="profile picture">

  </picture>

</figure>

</div>
<div class="person-info">
    <h2>
      T. Anderson
      
      Keller
    </h2>
    <p><b>PhD candidate
    
    (advised by Max Welling)
    
    </b><br>
    
    AMLab, Delta Lab<br>
    
    
    Institute of Informatics<br>
    
    
    University of Amsterdam<br>
    
    
    Science Park 904, Room C3.201<br>
    
    </p>

    <p>
        <i class="far fa-envelope"></i> <a href="mailto:tchie6Aetachie6AetkellervaraiRi8uvachie6Aetnl" onmouseover="this.href=this.href.replace(/varaiRi8/,'@').replace(/chie6Aet/g,'.').replace(/Oe8eed6M/g,'_').replace(/oaPoo7eo/g,'-')"><span class="o3m41l" data-x="t.a.keller" data-y="uva.nl"></span></a>
        
    </p>

    
    <i class="far fa-address-card"></i> <a href="https://akandykeller.github.io/" title="Work" target="_blank" rel="noopener noreferrer"> Personal page </a>  
         
    
    <i class="ai ai-google-scholar"></i> <a href="https://scholar.google.com/citations?user=Tb86kC0AAAAJ" title="Scholar" target="_blank" rel="noopener noreferrer">Google scholar </a> 
    
    
    <i class="fab fa-github"></i> <a href="https://github.com/akandykeller" title="GitHub" target="_blank" rel="noopener noreferrer"> Github </a> 
    
    
    <i class="fab fa-twitter"></i> <a href="https://twitter.com/t_andy_keller" title="Twitter" target="_blank" rel="noopener noreferrer"> Twitter </a> 
     
</div>

<p>My first name is Thomas, but most people call me by my middle name – Andy. I’m currently a third-year PhD student supervised by Max Welling at the University of Amsterdam. My work is focused on unsupervised structured representation learning, inspired and supported by observations from neuroscience. In pursuit of this goal, during my PhD I have developed novel methods for probabilistic generative modeling which make use of biologically plausible mechanisms such as learned feedback connections and topographic organization to approximate otherwise analytically intractible solutions. In the long term, I hope to be able to answer the question of how transformations and invariances are learned and encoded in the brain, and further understand how the 2-dimensional structure of the cortical surface shapes how learning proceeds. More immediately, my current interests broadly include: developing unsupervised methods for learning approximately equivariant &amp; invariant representations, exploring the computational benefits of topographically organized representations, and improving techniques for efficiently training deep latent variable models.</p>


<br>



</div>


<div class="publications">
<h2>Selected Publications</h2>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="keller2021modeling" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Modeling Category-Selective Cortical Regions with Topographic Variational Autoencoders</div>
          <!-- Author -->
          <div class="author">Keller, T. Anderson, Gao, Qinghe, and Welling, Max
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In SVRHM 2021 Workshop at NeurIPS</em> Dec 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2110.13911.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Category-selectivity in the brain describes the observation that certain spatially localized areas of the cerebral cortex tend to respond robustly and selectively to stimuli from specific limited categories. One of the most well known examples of category-selectivity is the Fusiform Face Area (FFA), an area of the inferior temporal cortex in primates which responds preferentially to images of faces when compared with objects or other generic stimuli. In this work, we leverage the newly introduced Topographic Variational Autoencoder to model the emergence of such localized category-selectivity in an unsupervised manner. Experimentally, we demonstrate our model yields spatially dense neural clusters selective to faces, bodies, and places through visualized maps of Cohen’s d metric. We compare our model with related supervised approaches, namely the Topographic Deep Artificial Neural Network (TDANN) of Lee et al., and discuss both theoretical and empirical similarities. Finally, we show preliminary results suggesting that our model yields a nested spatial hierarchy of increasingly abstract categories, analogous to observations from the human ventral temporal cortex.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICCV</abbr></div>

        <!-- Entry bib key -->
        <div id="Keller_2021_ICCV" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Predictive Coding With Topographic Variational Autoencoders</div>
          <!-- Author -->
          <div class="author">Keller, T. Anderson, and Welling, Max
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</em> Oct 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Keller_Predictive_Coding_With_Topographic_Variational_Autoencoders_ICCVW_2021_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Category-selectivity in the brain describes the observation that certain spatially localized areas of the cerebral cortex tend to respond robustly and selectively to stimuli from specific limited categories. One of the most well known examples of category-selectivity is the Fusiform Face Area (FFA), an area of the inferior temporal cortex in primates which responds preferentially to images of faces when compared with objects or other generic stimuli. In this work, we leverage the newly introduced Topographic Variational Autoencoder to model of the emergence of such localized category-selectivity in an unsupervised manner. Experimentally, we demonstrate our model yields spatially dense neural clusters selective to faces, bodies, and places through visualized maps of Cohen’s d metric. We compare our model with related supervised approaches, namely the TDANN, and discuss both theoretical and empirical similarities. Finally, we show preliminary results suggesting that our model yields a nested spatial hierarchy of increasingly abstract categories, analogous to observations from the human ventral temporal cortex.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="NEURIPS2021_f03704cb" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Topographic VAEs learn Equivariant Capsules</div>
          <!-- Author -->
          <div class="author">Keller, T. Anderson, and Welling, Max
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> Oct 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://proceedings.neurips.cc/paper/2021/file/f03704cb51f02f80b09bffba15751691-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this work we seek to bridge the concepts of topographic organization and equivariance in neural networks. To accomplish this, we introduce the Topographic VAE: a novel method for efficiently training deep generative models with topographically organized latent variables. We show that such a model indeed learns to organize its activations according to salient characteristics such as digit class, width, and style on MNIST. Furthermore, through topographic organization over time (i.e. temporal coherence), we demonstrate how predefined latent space transformation operators can be encouraged for observed transformed input sequences – a primitive form of unsupervised learned equivariance. We demonstrate that this model successfully learns sets of approximately equivariant features (i.e. "capsules") directly from sequences and achieves higher likelihood on correspondingly transforming test sequences. Equivariance is verified quantitatively by measuring the approximate commutativity of the inference network and the sequence transformations. Finally, we demonstrate approximate equivariance to complex transformations, expanding upon the capabilities of existing group equivariant neural networks.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="pmlr-v139-keller21a" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Self Normalizing Flows</div>
          <!-- Author -->
          <div class="author">Keller, T. Anderson, Peters, Jorn W.T., Jaini, Priyank, Hoogeboom, Emiel, Forré, Patrick, and Welling, Max
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 38th International Conference on Machine Learning</em> 18–24 jul 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://proceedings.mlr.press/v139/keller21a/keller21a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Efficient gradient computation of the Jacobian determinant term is a core problem in many machine learning settings, and especially so in the normalizing flow framework. Most proposed flow models therefore either restrict to a function class with easy evaluation of the Jacobian determinant, or an efficient estimator thereof. However, these restrictions limit the performance of such density models, frequently requiring significant depth to reach desired performance levels. In this work, we propose \emphSelf Normalizing Flows, a flexible framework for training normalizing flows by replacing expensive terms in the gradient by learned approximate inverses at each layer. This reduces the computational complexity of each layer’s exact update from \mathcalO(D^3) to \mathcalO(D^2), allowing for the training of flow architectures which were otherwise computationally infeasible, while also providing efficient sampling. We show experimentally that such models are remarkably stable and optimize to similar data likelihood values as their exact gradient counterparts, while training more quickly and surpassing the performance of functionally constrained counterparts.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:journals/corr/abs-2106-15577" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">As easy as APC: Leveraging self-supervised learning in the context
               of time series classification with varying levels of sparsity and
               severe class imbalance</div>
          <!-- Author -->
          <div class="author">Wever, Fiorella, Keller, T. Anderson, Garcia, Victor, and Symul, Laura
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Self-Supervised Learning Workshop at NeurIPS</em> 18–24 jul 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2106.15577.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>High levels of missing data and strong class imbalance are ubiquitous challenges that are often presented simultaneously in real-world time series data. Existing methods approach these problems separately, frequently making significant assumptions about the underlying data generation process in order to lessen the impact of missing information. In this work, we instead demonstrate how a general self-supervised training method, namely Autoregressive Predictive Coding (APC), can be leveraged to overcome both missing data and class imbalance simultaneously without strong assumptions. Specifically, on a synthetic dataset, we show that standard baselines are substantially improved upon through the use of APC, yielding the greatest gains in the combined setting of high missingness and severe class imbalance. We further apply APC on two real-world medical time-series datasets, and show that APC improves the classification performance in all settings, ultimately achieving state-of-the-art AUPRC results on the Physionet benchmark</p>
          </div>
        </div>
      </div>
</li>
</ol>
</div>
 

</div>

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2022 AMLab  | Amsterdam Machine Learning Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

