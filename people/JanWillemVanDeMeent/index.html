<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>AMLab  | Amsterdam Machine Learning Lab | Janwillemvandemeent</title>
    <meta name="author" content="AMLab  | Amsterdam Machine Learning Lab" />
    <meta name="description" content="Dr. Jan-Willem van de Meent is an Associate Professor (Universitair Hoofddocent) at the University of Amsterdam. He directs the [AMLab](https://amlab.science.uva.nl/), co-directs the [Uva Bosch Delta Lab](https://ivi.fnwi.uva.nl/uvaboschdeltalab/), and directs the [Amsterdam ELLIS Unit](https://ivi.fnwi.uva.nl/ellis/). He previously held a position as an Assistant Professor at Northeastern University, where he continues to co-advise and collaborate. Prior to becoming faculty at Northeastern, he held a postdoctoral position with Frank Wood at Oxford, as well as a postdoctoral position with Chris Wiggins and Ruben Gonzalez at Columbia University. He carried out his PhD research in biophysics at Leiden and Cambridge with Wim van Saarloos and Ray Goldstein. He served as a founding co-chair of the international conference on probabilistic programming ([PROBPROG](https://probprog.cc/)) and served as a program chair for the international conference on artificial intelligence and statistics ([AISTATS](https://aistats.org/aistats2023/)). He was the recipient of numerous grants, including an NWO Rubicon Fellowship and of an NSF CAREER award. 

Jan-Willem van de Meent’s research seeks to understand what methods in AI have the potential to generalize across diverse application domains, and how we can think compositionally about such methods. One aspect of his work focuses on methods development in generative AI, deep learning, and probabilistic programming. He also collaborates extensively in a range of application domains. In the past he has worked on problems in biophysics, neuroscience, healthcare, and robotics. His current collaborations focus on physical chemistry, fluid mechanics, and materials science. The two problems he currently cares about most are uses of AI to make scientific computation more scalable, and maximizing data-efficiency of AI methods in the context of scientific domains. 
" />
    <meta name="keywords" content="machine learning research" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/AMLab-logo.svg"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://amlab-amsterdam.github.io/people/JanWillemVanDeMeent/">
    
    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://AMLab-Amsterdam.github.io/"><span class="font-weight-bold">AMLab</span>   | Amsterdam Machine Learning Lab</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li> -->
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/contact/">Contact</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/joining/">Joining</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/people/">People</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- person-page.html -->
<div class="post">

<!--   <header class="post-header">
    <h1 class="post-title">JanWillemVanDeMeent.md</h1>
    <p class="post-description">AI for scalable and data-efficient scientific computation.</p>
  </header>
 -->
  <article>
    <div class="clearfix">
<div class="person-header">
<div class="person-headshot float-left">
    <figure>

  <picture>
<!--     <source media="(max-width: 480px)" srcset="/assets/img/JanWillemVanDeMeent-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/JanWillemVanDeMeent-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/JanWillemVanDeMeent-1400.webp" />
    -->
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/JanWillemVanDeMeent.jpg" title="profile picture">

  </picture>

</figure>

</div>
<div class="person-info">
    <h2>
      Jan-Willem
      
      van de Meent
    </h2>
    <p><b>Associate professor (UHD)
    
    </b><br>
    
    AMLab and Delta Lab<br>
    
    
    Informatics Institute<br>
    
    
    University of Amsterdam<br>
    
    
    Science Park, Lab 42, L4.13<br>
    
    </p>

    <p>
        <i class="far fa-envelope"></i> <a href="mailto:jchie6Aetwchie6AetvandemeentvaraiRi8uvachie6Aetnl" onmouseover="this.href=this.href.replace(/varaiRi8/,'@').replace(/chie6Aet/g,'.').replace(/Oe8eed6M/g,'_').replace(/oaPoo7eo/g,'-')"><span class="o3m41l" data-x="j.w.vandemeent" data-y="uva.nl"></span></a>
        
    </p>

         
    
    <i class="ai ai-google-scholar"></i> <a href="https://scholar.google.com/citations?user=aCGsfUAAAAAJ" title="Scholar" target="_blank" rel="noopener noreferrer">Google scholar </a> 
    
    
    <i class="fab fa-github"></i> <a href="https://github.com/jwvdm" title="GitHub" target="_blank" rel="noopener noreferrer"> Github </a> 
    
    
        
            <i class="fab fa-mastodon"></i> <a rel="me noopener noreferrer" href="https://ellis.social/@jwvdm" title="Mastodon" target="_blank"> Mastodon </a> 
         
     
    
    <i class="fab fa-twitter"></i> <a href="https://twitter.com/jwvdm" title="Twitter" target="_blank" rel="noopener noreferrer"> Twitter </a> 
     
</div>

<p>Dr. Jan-Willem van de Meent is an Associate Professor (Universitair Hoofddocent) at the University of Amsterdam. He directs the <a href="https://amlab.science.uva.nl/" target="_blank" rel="noopener noreferrer">AMLab</a>, co-directs the <a href="https://ivi.fnwi.uva.nl/uvaboschdeltalab/" target="_blank" rel="noopener noreferrer">Uva Bosch Delta Lab</a>, and directs the <a href="https://ivi.fnwi.uva.nl/ellis/" target="_blank" rel="noopener noreferrer">Amsterdam ELLIS Unit</a>. He previously held a position as an Assistant Professor at Northeastern University, where he continues to co-advise and collaborate. Prior to becoming faculty at Northeastern, he held a postdoctoral position with Frank Wood at Oxford, as well as a postdoctoral position with Chris Wiggins and Ruben Gonzalez at Columbia University. He carried out his PhD research in biophysics at Leiden and Cambridge with Wim van Saarloos and Ray Goldstein. He served as a founding co-chair of the international conference on probabilistic programming (<a href="https://probprog.cc/" target="_blank" rel="noopener noreferrer">PROBPROG</a>) and served as a program chair for the international conference on artificial intelligence and statistics (<a href="https://aistats.org/aistats2023/" target="_blank" rel="noopener noreferrer">AISTATS</a>). He was the recipient of numerous grants, including an NWO Rubicon Fellowship and of an NSF CAREER award.</p>

<p>Jan-Willem van de Meent’s research seeks to understand what methods in AI have the potential to generalize across diverse application domains, and how we can think compositionally about such methods. One aspect of his work focuses on methods development in generative AI, deep learning, and probabilistic programming. He also collaborates extensively in a range of application domains. In the past he has worked on problems in biophysics, neuroscience, healthcare, and robotics. His current collaborations focus on physical chemistry, fluid mechanics, and materials science. The two problems he currently cares about most are uses of AI to make scientific computation more scalable, and maximizing data-efficiency of AI methods in the context of scientific domains.</p>


<br>



</div>

 



<div class="publications">
<h2>Recent Publications</h2>

<h3>2025</h3>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="guzmancordero2025exponential" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Controlled Generation with Equivariant Variational Flow Matching</div>
          <!-- Author -->
          <div class="author">Eijkelboom, Floor, Zimmermann, Heiko, Bekkers, Erik, Welling, Max, Naesseth, Christian, and van de Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In International Conference on Machine Learning</em> 2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="guzmancordero2025exponentiam" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Exponential Family Variational Flow Matching for Tabular Data Generation</div>
          <!-- Author -->
          <div class="author">Guzmán-Cordero*, Andrés, Eijkelboom*, Floor, and van de Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In International Conference on Machine Learning</em> 2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="zhdanov2025erwin" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems</div>
          <!-- Author -->
          <div class="author">Zhdanov, Maksim, Welling, Max, and van de Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In International Conference on Machine Learning</em> 2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/abs/2502.17019" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://arxiv.org/pdf/2502.17019" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Large-scale physical systems defined on irregular grids pose significant scalability challenges for deep learning methods, especially in the presence of long-range interactions and multi-scale coupling. Traditional approaches that compute all pairwise interactions, such as attention, become computationally prohibitive as they scale quadratically with the number of nodes. We present Erwin, a hierarchical transformer inspired by methods from computational many-body physics, which combines the efficiency of tree-based algorithms with the expressivity of attention mechanisms. Erwin employs ball tree partitioning to organize computation, which enables linear-time attention by processing nodes in parallel within local neighborhoods of fixed size. Through progressive coarsening and refinement of the ball tree structure, complemented by a novel cross-ball interaction mechanism, it captures both fine-grained local details and global features. We demonstrate Erwin’s effectiveness across multiple domains, including cosmology, molecular dynamics, and particle fluid dynamics, where it consistently outperforms baseline methods both in accuracy and computational efficiency.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">PRL</abbr></div>

        <!-- Entry bib key -->
        <div id="dijkman2025learning" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Learning Neural Free-Energy Functionals with Pair-Correlation Matching</div>
          <!-- Author -->
          <div class="author">Dijkman, Jacobus, Dijkstra, Marjolein, van Roij, René, Welling, Max, van de Meent, Jan-Willem, and Ensing, Bernd
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Physical Review Letters</em> Feb 2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.134.056103" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://arxiv.org/pdf/2403.15007" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The intrinsic Helmholtz free-energy functional, the centerpiece of classical density functional theory, is at best only known approximately for 3D systems. Here we introduce a method for learning a neural-network approximation of this functional by exclusively training on a dataset of radial distribution functions, circumventing the need to sample costly heterogeneous density profiles in a wide variety of external potentials. For a supercritical Lennard-Jones system with planar symmetry, we demonstrate that the learned neural free-energy functional accurately predicts inhomogeneous density profiles under various complex external potentials obtained from simulations.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICRA</abbr></div>

        <!-- Entry bib key -->
        <div id="biza25onrobot" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">On-Robot Reinforcement Learning with Goal-Contrastive Rewards</div>
          <!-- Author -->
          <div class="author">Biza, Ondrej, Weng, Thomas, Sun, Lingfeng, Schmeckpeper, Karl, Kelestemur, Tarik, Ma, Yecheng Jason, Platt, Robert, van de Meent, Jan-Willem, and Wong, Lawson L. S.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 2025 IEEE International Conference on Robotics and Automation, ICRA’25</em> Feb 2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2410.19989" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://arxiv.org/pdf/2410.19989" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
</ol>

<h3>2024</h3>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="kunze2024practical" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Practical Shuffle Coding</div>
          <!-- Author -->
          <div class="author">Kunze, Julius, Severo, Daniel, van de Meent, Jan-Willem, and Townsend, James
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/98d17a9632e1534bae96793e99dc3c2d-Abstract-Conference.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/98d17a9632e1534bae96793e99dc3c2d-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="eijkelboom2024variational" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Variational Flow Matching for Graph Generation</div>
          <!-- Author -->
          <div class="author">Eijkelboom, Floor, Bartosh, Grigory, Naesseth, Christian A., Welling, Max, and van de Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/15b780350b302a1bf9a3bd273f5c15a4-Abstract-Conference.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/15b780350b302a1bf9a3bd273f5c15a4-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="zimmermann2024visa" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">VISA: Variational Inference with Sequential Sample-Average Approximations</div>
          <!-- Author -->
          <div class="author">Zimmermann, Heiko, Naesseth, Christian A., and van de Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/fa948624dfde013671e72c1a7ca4aebc-Abstract-Conference.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/fa948624dfde013671e72c1a7ca4aebc-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NAACL</abbr></div>

        <!-- Entry bib key -->
        <div id="mcinerney2024reducing" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Towards Reducing Diagnostic Errors with Interpretable Risk Prediction</div>
          <!-- Author -->
          <div class="author">McInerney, Denis Jered, Dickinson, William, Flynn, Lucy C., Young, Andrea C., Young, Geoffrey S., van de Meent, Jan-Willem, and Wallace, Byron C.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://aclanthology.org/2024.naacl-long.399/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://aclanthology.org/2024.naacl-long.399.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model. We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div>

        <!-- Entry bib key -->
        <div id="kunze2024entropy" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Entropy Coding of Unordered Data Structures</div>
          <!-- Author -->
          <div class="author">Kunze, Julius, Severo, Daniel, Zani, Giulio, van de Meent, Jan-Willem, and Townsend, James
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In International Conference on Learning Representations (ICLR)</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/forum?id=PggJ9CbEN7" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://openreview.net/pdf?id=PggJ9CbEN7" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
</ol>

<h3>2023</h3>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div>

        <!-- Entry bib key -->
        <div id="mcinerney2023chill" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models</div>
          <!-- Author -->
          <div class="author">McInerney, Denis Jered, Young, Geoffrey, Meent, Jan-Willem, and Wallace, Byron
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In The 2023 Conference on Empirical Methods in Natural Language Processing</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/forum?id=TSdWY9GaHA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://openreview.net/pdf?id=TSdWY9GaHA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="esmaeili2023topological" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Topological Obstructions and How to Avoid Them</div>
          <!-- Author -->
          <div class="author">Esmaeili, Babak, Walters, Robin, Zimmermann, Heiko, and van de Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Thirty-seventh Conference on Neural Information Processing Systems (to appear)</em> Dec 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/1c12ccfc7720f6b680edea17300bfc2b-Abstract-Conference.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/1c12ccfc7720f6b680edea17300bfc2b-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">CoRL</abbr></div>

        <!-- Entry bib key -->
        <div id="biza2023oneshot" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">One-shot Imitation Learning via Interaction Warping</div>
          <!-- Author -->
          <div class="author">Biza, Ondrej, Thompson, Skye, Pagidi, Kishore Reddy, Kumar, Abhinav, Pol, Elise, Walters, Robin, Kipf, Thomas, Meent, Jan-Willem, Wong, Lawson L.S., and Platt, Robert
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 7th Annual Conference on Robot Learning</em> Nov 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://proceedings.mlr.press/v229/biza23a.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://proceedings.mlr.press/v229/biza23a/biza23a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ACT</abbr></div>

        <!-- Entry bib key -->
        <div id="sennesh2023string" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">String Diagrams with Factorized Densities</div>
          <!-- Author -->
          <div class="author">Sennesh, Eli, and van de Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Applied Category Theory</em> Jul 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openreview.net/forum?id=tfNdfCEWy2" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://arxiv.org/pdf/2305.02506" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>A growing body of research on probabilistic programs and causal models has highlighted the need to reason compositionally about model classes that extend directed graphical models. Both probabilistic programs and causal models define a joint probability density over a set of random variables, and exhibit sparse structure that can be used to reason about causation and conditional independence. This work builds on recent work on Markov categories of probabilistic mappings to define a category whose morphisms combine a joint density, factorized over each sample space, with a deterministic mapping from samples to return values. This is a step towards closing the gap between recent category-theoretic descriptions of probability measures, and the operational definitions of factorized densities that are commonly employed in probabilistic programming and causal inference.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML WNC</abbr></div>

        <!-- Entry bib key -->
        <div id="kunze2023entropy" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Entropy Coding of Unordered Data Structures</div>
          <!-- Author -->
          <div class="author">Kunze, Julius, Severo, Daniel, Zani, Giulio, Meent, Jan-Willem, and Townsend, James
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ICML 2023 Workshop Neural Compression: From Information Theory to Applications</em> Jul 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TMLR</abbr></div>

        <!-- Entry bib key -->
        <div id="zimmermann2023variational" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Variational Perspective on Generative Flow Networks</div>
          <!-- Author -->
          <div class="author">Zimmermann, Heiko, Lindsten, Fredrik, Meent, Jan-Willem, and Naesseth, Christian A
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Transactions on Machine Learning Research</em> Apr 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openreview.net/forum?id=AZ4GobeSLq" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://openreview.net/pdf?id=AZ4GobeSLq" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/zmheiko/variational-perspective-on-gflownets" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Generative flow networks (GFNs) are a class of probabilistic models for sequential sampling of composite objects, proportional to a target distribution that is defined in terms of an energy function or a reward. GFNs are typically trained using a flow matching or trajectory balance objective, which matches forward and backward transition models over trajectories. In this work we introduce a variational objective for training GFNs, which is a convex combination of the reverse- and forward KL divergences, and compare it to the trajectory balance objective when sampling from the forward- and backward model, respectively. We show that, in certain settings, variational inference for GFNs is equivalent to minimizing the trajectory balance objective, in the sense that both methods compute the same score-function gradient. This insight suggests that in these settings, control variates, which are commonly used to reduce the variance of score-function gradient estimates, can also be used with the trajectory balance objective. We evaluate our findings and the performance of the proposed variational objective numerically by comparing it to the trajectory balance objective on two synthetic tasks.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">LAFI</abbr></div>

        <!-- Entry bib key -->
        <div id="townsend_lafi_2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Verified Reversible Programming for Verified Lossless Compression</div>
          <!-- Author -->
          <div class="author">Townsend, James, and van de Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In POPL Workshop on Languages for Inference (LAFI)</em> Apr 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://popl23.sigplan.org/details/lafi-2023-papers/2/Verified-Reversible-Programming-for-Verified-Lossless-Compression" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://arxiv.org/pdf/2211.09676.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">LAFI</abbr></div>

        <!-- Entry bib key -->
        <div id="sennesh_lafi_2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A convenient category of tracing measure kernels</div>
          <!-- Author -->
          <div class="author">Sennesh, Eli, and van de Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In POPL Workshop on Languages for Inference (LAFI)</em> Apr 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://popl23.sigplan.org/details/lafi-2023-papers/8/A-convenient-category-of-tracing-measure-kernels" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://popl23.sigplan.org/details?action-call-with-get-request-type=1&amp;e045187d36474ff29b18a21bf9aab73aaction_174265066101788833e464a9a6ab51286f3e5105d96=1&amp;__ajax_runtime_request__=1&amp;context=POPL-2023&amp;track=lafi-2023-papers&amp;urlKey=8&amp;decoTitle=A-convenient-category-of-tracing-measure-kernels" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">LAFI</abbr></div>

        <!-- Entry bib key -->
        <div id="lew_lafi_2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Semantics of Probabilistic Program Traces</div>
          <!-- Author -->
          <div class="author">Lew, Alexander, Sennesh, Eli, van de Meent, Jan-Willem, and Mansinghka, Vikash
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In POPL Workshop on Languages for Inference (LAFI)</em> Apr 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://popl23.sigplan.org/details/lafi-2023-papers/1/Semantics-of-Probabilistic-Program-Traces" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://popl23.sigplan.org/details?action-call-with-get-request-type=1&amp;173b82200a714176bf311ea4c21818eeaction_17426506610d2b6f0130614628d0da5e92d54a87dc5=1&amp;__ajax_runtime_request__=1&amp;context=POPL-2023&amp;track=lafi-2023-papers&amp;urlKey=1&amp;decoTitle=Semantics-of-Probabilistic-Program-Traces" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
</ol>

<h3>2022</h3>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div>

        <!-- Entry bib key -->
        <div id="mcinerney2022that" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">That’s the Wrong Lung! Evaluating and Improving the Interpretability of Unsupervised Multimodal Encoders for Medical Data</div>
          <!-- Author -->
          <div class="author">McInerney, Denis Jered, Young, Geoffrey, van de Meent, Jan-Willem, and Wallace, Byron C.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing</em> Dec 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Pretraining multimodal models on Electronic Health Records (EHRs) provides a means of learning representations that can transfer to downstream tasks with minimal supervision. Recent multimodal models induce soft local alignments between image regions and sentences. This is of particular interest in the medical domain, where alignments might highlight regions in an image relevant to specific phenomena described in free-text. While past work has suggested that attention “heatmaps” can be interpreted in this manner, there has been little evaluation of such alignments. We compare alignments from a state-of-the-art multimodal (image and text) model for EHR with human annotations that link image regions to sentences. Our main finding is that the text has an often weak or unintuitive influence on attention; alignments do not consistently reflect basic anatomical information. Moreover, synthetic modifications — such as substituting “left” for “right” — do not substantially influence highlights. Simple techniques such as allowing the model to opt out of attending to the image and few-shot finetuning show promise in terms of their ability to improve alignments with very little or no supervision. We make our code and checkpoints open-source.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS WSGNR</abbr></div>

        <!-- Entry bib key -->
        <div id="esmaeili2022understanding" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Understanding Optimization Challenges when Encoding to Geometric Structures</div>
          <!-- Author -->
          <div class="author">Esmaeili, Babak, Walters, Robin, Zimmermann, Heiko, and Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations</em> Dec 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">PLOS Comp Bio</abbr></div>

        <!-- Entry bib key -->
        <div id="smedemark-margulies2022probabilistic" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Probabilistic Program Inference in Network-Based Epidemiological Simulations</div>
          <!-- Author -->
          <div class="author">Smedemark-Margulies, Niklas, Walters, Robin, Zimmermann, Heiko, Laird, Lucas, Loo, Christian, Kaushik, Neela, Caceres, Rajmonda, and Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>PLOS Computational Biology</em> Nov 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010591" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1010591&amp;type=printable" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Accurate epidemiological models require parameter estimates that account for mobility patterns and social network structure. We demonstrate the effectiveness of probabilistic programming for parameter inference in these models. We consider an agent-based simulation that represents mobility networks as degree-corrected stochastic block models, whose parameters we estimate from cell phone co-location data. We then use probabilistic program inference methods to approximate the distribution over disease transmission parameters conditioned on reported cases and deaths. Our experiments demonstrate that the resulting models improve the quality of fit in multiple geographies relative to baselines that do not model network topology.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Bio. Pysch.</abbr></div>

        <!-- Entry bib key -->
        <div id="sennesh2022interoception" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Interoception as Modeling, Allostasis as Control</div>
          <!-- Author -->
          <div class="author">Sennesh, Eli, Theriault, Jordan, Brooks, Dana, Meent, Jan-Willem, Barrett, Lisa Feldman, and Quigley, Karen S.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Biological Psychology</em> Nov 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.1016/j.biopsycho.2021.108242" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://files.osf.io/v1/resources/2ymuj/providers/osfstorage/61c3782b72da2300d7bf9b50?action=download&amp;direct&amp;version=1" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The brain regulates the body by anticipating its needs and attempting to meet them before they arise – a process called allostasis. Allostasis requires a model of the changing sensory conditions within the body, a process called interoception. In this paper, we examine how interoception may provide performance feedback for allostasis. We suggest studying allostasis in terms of control theory, reviewing control theory’s applications to related issues in physiology, motor control, and decision making. We synthesize these by relating them to the important properties of allostatic regulation as a control problem. We then sketch a novel formalism for how the brain might perform allostatic control of the viscera by analogy to skeletomotor control, including a mathematical view on how interoception acts as performance feedback for allostasis. Finally, we suggest ways to test implications of our hypotheses.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Neuroinf.</abbr></div>

        <!-- Entry bib key -->
        <div id="khan2022computational" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Computational Neural Model for Mapping Degenerate Neural Architectures</div>
          <!-- Author -->
          <div class="author">Khan, Zulqarnain, Wang, Yiyu, Sennesh, Eli, Dy, Jennifer, Ostadabbas, Sarah, van de Meent, Jan-Willem, Hutchinson, J. Benjamin, and Satpute, Ajay B.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Neuroinformatics</em> Mar 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.1007/s12021-022-09580-9" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://link.springer.com/content/pdf/10.1007/s12021-022-09580-9.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Degeneracy in biological systems refers to a many-to-one mapping between physical structures and their functional (including psychological) outcomes. Despite the ubiquity of the phenomenon, traditional analytical tools for modeling degeneracy in neuroscience are extremely limited. In this study, we generated synthetic datasets to describe three situations of degeneracy in fMRI data to demonstrate the limitations of the current univariate approach. We describe a novel computational approach for the analysis referred to as neural topographic factor analysis (NTFA). NTFA is designed to capture variations in neural activity across task conditions and participants. The advantage of this discovery-oriented approach is to reveal whether and how experimental trials and participants cluster into task conditions and participant groups. We applied NTFA on simulated data, revealing the appropriate degeneracy assumption in all three situations and demonstrating NTFA’s utility in uncovering degeneracy. Lastly, we discussed the importance of testing degeneracy in fMRI data and the implications of applying NTFA to do so.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">WACV</abbr></div>

        <!-- Entry bib key -->
        <div id="Bateni2022_TransductiveCNAPS" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Enhancing Few-Shot Image Classification With Unlabelled Examples</div>
          <!-- Author -->
          <div class="author">Bateni, Peyman, Barber, Jarred, Meent, Jan-Willem, and Wood, Frank
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em> Jan 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.1109/WACV51458.2022.00166" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Bateni_Enhancing_Few-Shot_Image_Classification_With_Unlabelled_Examples_WACV_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/plai-group/simple-cnaps" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We develop a transductive meta-learning method that uses unlabelled instances to improve few-shot image classification performance. Our approach combines a regularized Mahalanobis-distance-based soft k-means clustering procedure with a modified state of the art neural adaptive feature extractor to achieve improved test-time classification accuracy using unlabelled data. We evaluate our method on transductive few-shot learning tasks, in which the goal is to jointly predict labels for query (test) examples given a set of support (training) examples. We achieve state of the art performance on the Meta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. All trained models and code have been made publicly available</p>
          </div>
        </div>
      </div>
</li>
</ol>

<h3>2021</h3>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AISTATS</abbr></div>

        <!-- Entry bib key -->
        <div id="bozkurt2021rateregularization" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Rate-Regularization and Generalization in Variational Autoencoders</div>
          <!-- Author -->
          <div class="author">Bozkurt, Alican, Esmaeili, Babak, Tristan, Jean-Baptiste, Brooks, Dana, Dy, Jennifer, and Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In International Conference on Artificial Intelligence and Statistics</em> Mar 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://proceedings.mlr.press/v130/bozkurt21a.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="http://proceedings.mlr.press/v130/bozkurt21a/bozkurt21a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Variational autoencoders (VAEs) optimize an objective that comprises a reconstruction loss (the distortion) and a KL term (the rate). The rate is an upper bound on the mutual information, which is...</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">UAI</abbr></div>

        <!-- Entry bib key -->
        <div id="stites-zimmerman2021LearningProposals" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Learning proposals for probabilistic programs with inference combinators</div>
          <!-- Author -->
          <div class="author">Stites, Sam, Zimmermann, Heiko, Wu, Hao, Sennesh, Eli, and Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence</em> 27–30 jul 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://proceedings.mlr.press/v161/stites21a.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://proceedings.mlr.press/v161/stites21a/stites21a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/probtorch/combinators" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang2021disentangling" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Disentangling Representations of Text by Masking Transformers</div>
          <!-- Author -->
          <div class="author">Zhang, Xiongyi, van de Meent, Jan-Willem, and Wallace, Byron
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em> Nov 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://dx.doi.org/10.18653/v1/2021.emnlp-main.60" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://aclanthology.org/2021.emnlp-main.60.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Representations from large pretrained models such as BERT encode a range of features into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspects. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, toxicity from dialect in Tweets, and syntax from semantics. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as — and often better than —previously proposed methods based on variational autoencoders and adversarial training.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NAACL</abbr></div>

        <!-- Entry bib key -->
        <div id="amir2021impact" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">On the Impact of Random Seeds on the Fairness of Clinical Classifiers</div>
          <!-- Author -->
          <div class="author">Amir, Silvio, van de Meent, Jan-Willem, and Wallace, Byron
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em> Jun 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://dx.doi.org/10.18653/v1/2021.naacl-main.299" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://aclanthology.org/2021.naacl-main.299.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent work has shown that fine-tuning large networks is surprisingly sensitive to changes in random seed(s). We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III —— the standard dataset in clinical NLP research. Apparent subgroup performance varies substantially for seeds that yield similar overall performance, although there is no evidence of a trade-off between overall and subgroup performance. However, we also find that the small sample sizes inherent to looking at intersections of minority groups and somewhat rare conditions limit our ability to accurately estimate disparities. Further, we find that jointly optimizing for high overall performance and low disparities does not yield statistically significant improvements. Our results suggest that fairness work using MIMIC-III should carefully account for variations in apparent differences that may arise from stochasticity and small sample sizes.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="zimmermann2021nested" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Nested Variational Inference</div>
          <!-- Author -->
          <div class="author">Zimmermann, Heiko, Wu, Hao, Esmaeili, Babak, and Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> Jun 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://proceedings.neurips.cc/paper/2021/hash/ab49b208848abe14418090d95df0d590-Abstract.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://proceedings.neurips.cc/paper/2021/file/ab49b208848abe14418090d95df0d590-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We develop nested variational inference (NVI), a family of methods that learn proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristics to guide the sampler. Our experiments apply NVI to (a) sample from a multimodal distribution using a learned annealing path (b) learn heuristics that approximate the likelihood of future observations in a hidden Markov model and (c) to perform amortized inference in hierarchical deep generative models. We observe that optimizing nested objectives leads to improved sample quality in terms of log average weight and effective sample size.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AAMAS</abbr></div>

        <!-- Entry bib key -->
        <div id="biza2021action" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Action Priors for Large Action Spaces in Robotics</div>
          <!-- Author -->
          <div class="author">Biza, Ondrej, Wang, Dian, Platt, Robert, Meent, Jan-Willem, and Wong, Lawson L.S.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems</em> Jun 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://dl.acm.org/doi/10.5555/3463952.3463982" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p205.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/ondrejba/action_priors" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In robotics, it is often not possible to learn useful policies using pure model-free reinforcement learning without significant reward shaping or curriculum learning. As a consequence, many researchers rely on expert demonstrations to guide learning. However, acquiring expert demonstrations can be expensive. This paper proposes an alternative approach where the solutions of previously solved tasks are used to produce an action prior that can facilitate exploration in future tasks. The action prior is a probability distribution over actions that summarizes the set of policies found solving previous tasks. Our results indicate that this approach can be used to solve robotic manipulation problems that would otherwise be infeasible without expert demonstrations. Source code is available at https://github.com/ondrejba/action_priors.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="wu2021conjugate" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Conjugate Energy-Based Models</div>
          <!-- Author -->
          <div class="author">Wu*, Hao, Esmaeili*, Babak, Wick, Michael L, Tristan, Jean-Baptiste, and van de Meent, Jan-Willem
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In International Conference on Machine Learning</em> Jun 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this paper, we propose conjugate energy-based models (CEBMs), a new class of energy-based models that define a joint density over data and latent variables. The joint density of a CEBM decomposes into an intractable distribution over data and a tractable posterior over latent variables. CEBMs have similar use cases as variational autoencoders, in the sense that they learn an unsupervised mapping from data to latent variables. However, these models omit a generator network, which allows them to learn more flexible notions of similarity between data points. Our experiments demonstrate that conjugate EBMs achieve competitive results in terms of image modelling, predictive power of latent space, and out-of-domain detection on a variety of datasets.</p>
          </div>
        </div>
      </div>
</li>
</ol>

</div>

</div>

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 AMLab  | Amsterdam Machine Learning Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

