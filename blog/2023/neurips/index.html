<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>AMLab  | Amsterdam Machine Learning Lab | AMLab Papers at NeurIPS 2023</title>
    <meta name="author" content="AMLab  | Amsterdam Machine Learning Lab" />
    <meta name="description" content="Read here about the 8 papers that AMLab members will be presenting at NeurIPS this year. 
" />
    <meta name="keywords" content="machine learning research" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/AMLab-logo.svg"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://amlab-amsterdam.github.io/blog/2023/neurips/">
    
    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://AMLab-Amsterdam.github.io/"><span class="font-weight-bold">AMLab</span>   | Amsterdam Machine Learning Lab</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li> -->
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/contact/">Contact</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/joining/">Joining</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/people/">People</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">AMLab Papers at NeurIPS 2023</h1>
    <p class="post-meta">November 20, 2023, Max Zhdanov</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>

    </p>
  </header>

  <article class="post-content">
    <div style="background-color: rgba(123, 104, 238, 0.1); padding: 10px 0; text-align: center;">
    <img src="https://neurips.cc/media/Press/NeurIPS_logo.svg" alt="NeurIPS" width="240" height="120" style="margin: auto;">
</div>

<hr style="margin-top: 20px;">

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Clifford Group Equivariant Neural Networks</strong><br>
      <em>David Ruhe, Johannes Brandstetter, Patrick Forré</em><br>

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://openreview.net/forum?id=n84bzMrGUD&amp;referrer=%5BAuthor%20Console%5D(%2Fgroup%5DNeurIPS.cc%2F2023%2FConference%2FAuthors%23your-submissions)" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">Conference paper</a> · 
        <a href="https://openreview.net/pdf?id=n84bzMrGUD" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">PDF</a> · 
        <a href="https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">GitHub repo</a>
      </span>

      <strong>Abstract:</strong> We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing O(n)- and E(n)-equivariant models. We identify and study the Clifford group: a subgroup inside the Clifford algebra tailored to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, including their grade projections, constitutes an equivariant map with respect to the Clifford group, allowing us to parameterize equivariant neural network layers. An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a single core implementation, state-of-the-art performance on several distinct tasks, including a three-dimensional N-body experiment, a four-dimensional Lorentz-equivariant high-energy physics experiment, and a five-dimensional convex hull experiment. <br>

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #GeometricAlgebra #Equivariance #RepresentationTheory
      </span>

    </td>
    <td style="position: relative; width: 50%;">
      <img src="/assets/neurips2023/clifford.png" alt="Graphical Abstract" style="height: auto; max-height: 90%; width: auto; max-width: 90%;  margin-left: 5%;">
      <span style="position: absolute; top: 14%; right: 0; background-color: #ffcc00; color: black; padding: 5px; font-size: 20px; transform: rotate(35deg); transform-origin: top right;">
        Oral Presentation
      </span>
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;">

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Rotating Features for Object Discovery</strong><br>
      <em>Sindy Löwe, Phillip Lippe, Francesco Locatello, Max Welling</em><br>

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://arxiv.org/abs/2306.00600" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">Paper</a> · 
        <a href="https://arxiv.org/pdf/2306.00600.pdf" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">PDF</a> · 
        <a href="https://github.com/loeweX/RotatingFeatures" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">GitHub repo</a>
      </span>

      <strong>Abstract:</strong> The binding problem in human cognition, concerning how the brain represents and connects objects within a fixed network of neural connections, remains a subject of intense debate. Most machine learning efforts addressing this issue in an unsupervised setting have focused on slot-based methods, which may be limiting due to their discrete nature and difficulty to express uncertainty. Recently, the Complex AutoEncoder was proposed as an alternative that learns continuous and distributed object-centric representations. However, it is only applicable to simple toy data. In this paper, we present Rotating Features, a generalization of complex-valued features to higher dimensions, and a new evaluation procedure for extracting objects from distributed representations. Additionally, we show the applicability of our approach to pre-trained features. Together, these advancements enable us to scale distributed object-centric representations from simple toy to real-world data. We believe this work advances a new paradigm for addressing the binding problem in machine learning and has the potential to inspire further innovation in the field. <br>

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #ObjectDiscovery #RepresentationLearning #ComplexAutoEncoder
      </span>

    </td>
    <td style="position: relative; width: 50%;">
      <img src="https://github.com/loeweX/RotatingFeatures/blob/main/imgs/RotatingFeatures.png?raw=true" alt="Graphical Abstract" style="height: auto; max-height: 90%; width: auto; max-width: 90%; margin-left: 5%;">
      <span style="position: absolute; top: 14%; right: 0; background-color: #ffcc00; color: black; padding: 5px; font-size: 20px; transform: rotate(35deg); transform-origin: top right;">
        Oral Presentation
      </span>
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;">

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Deep Gaussian Markov Random Fields for graph-structured dynamical systems</strong><br>
      <em>Fiona Lippert, Bart Kranstauber, E. Emiel van Loon, Patrick Forré</em><br>

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://arxiv.org/abs/2306.08445" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">Conference paper</a> · 
        <a href="https://arxiv.org/pdf/2306.08445.pdf" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">PDF</a>
      </span>

      <strong>Abstract:</strong> Probabilistic inference in high-dimensional state-space models is computationally challenging. For many spatiotemporal systems, however, prior knowledge about the dependency structure of state variables is available. We leverage this structure to develop a computationally efficient approach to state estimation and learning in graph-structured state-space models with (partially) unknown dynamics and limited historical data. Building on recent methods that combine ideas from deep learning with principled inference in Gaussian Markov random fields (GMRF), we reformulate graph-structured state-space models as Deep GMRFs defined by simple spatial and temporal graph layers. This results in a flexible spatiotemporal prior that can be learned efficiently from a single time sequence via variational inference. Under linear Gaussian assumptions, we retain a closed-form posterior, which can be sampled efficiently using the conjugate gradient method, scaling favourably compared to classical Kalman filter based approaches. <br>

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #GaussianMarkovRandomFields #SpatiotemporalModels #VariationalInference
      </span>

    </td>
<td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/ST-DGMRF_graphical_abstract.png" alt="Graphical Abstract" style="height: auto; max-height: 80%; width: auto; max-width: 80%;  margin-left: 20%;">
    </td>
    
  </tr>
</table>

<hr style="margin-top: 20px;">

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Implicit Convolutional Kernels for Steerable CNNs</strong><br>
      <em>Maksim Zhdanov, Nico Hoffmann, Gabriele Cesa</em><br>
      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
    <a href="https://openreview.net/forum?id=2YtdxqvdjX" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">Conference paper</a> · 
    <a href="https://openreview.net/pdf?id=2YtdxqvdjX" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">PDF</a> · 
    <a href="https://github.com/maxxxzdn/implicit-steerable-cnns" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">GitHub repo</a>
  </span>

  <strong>Abstract:</strong> Steerable convolutional neural networks (CNNs) provide a general framework for building neural networks equivariant to translations and other transformations belonging to an origin-preserving group \(G\), such as reflections and rotations. They rely on standard convolutions with \(G\)-steerable kernels obtained by analytically solving the group-specific equivariance constraint imposed onto the kernel space. As the solution is tailored to a particular group \(G\), the implementation of a kernel basis does not generalize to other symmetry transformations, which complicates the development of general group equivariant models. We propose using implicit neural representation via multi-layer perceptrons (MLPs) to parameterize \(G\)-steerable kernels. The resulting framework offers a simple and flexible way to implement Steerable CNNs and generalizes to any group \(G\) for which a \(G\)-equivariant MLP can be built. We prove the effectiveness of our method on multiple tasks, including N-body simulations, point cloud classification, and molecular property prediction. <br>

  <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
    #SteerableCNNs #Equivariance #MolecularSimulations
  </span>

</td>
    <td style="width: 50%;">
      <img src="https://github.com/maxxxzdn/implicit-steerable-kernels/blob/main/assets/abstract.png?raw=true" alt="Graphical Abstract" style="height: auto; max-height: 60%; width: auto; max-width: 60%; margin-left: 20%;">
    </td>

  </tr>
</table>
<hr style="margin-top: 20px;">

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">The Memory-Perturbation Equation: Understanding Model's Sensitivity to Data</strong><br>
      <em>Peter Nickl, Lu Xu*, Dharmesh Tailor*, Thomas Möllenhoff, Emtiyaz Khan</em><br><em>(*=equal contribution)</em><br>

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://openreview.net/forum?id=dqS1GuoG2V" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">Conference paper</a> · 
        <a href="https://arxiv.org/abs/2310.19273" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">PDF</a> · 
        <a href="https://github.com/team-approx-bayes/memory-perturbation" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">GitHub repo</a>
      </span>

      <strong>Abstract:</strong> Understanding model's sensitivity to its training data is crucial but can also be challenging and costly, especially during training. To simplify such issues, we present the Memory-Perturbation Equation (MPE) which relates model's sensitivity to perturbation in its training data. Derived using Bayesian principles, the MPE unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. Our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data. The proposed equation is expected to be useful for future research on robust and adaptive learning. <br>

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #MemoryPerturbationEquation #Sensitivity #Generalization
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/mpe_blog_image.png" alt="Graphical Abstract" style="height: auto; max-height: 60%; width: auto; max-width: 60%;  margin-left: 20%;">
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;">

<table>

  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Topological Obstructions and How to Avoid Them</strong><br>
      <em>Babak Esmaeili, Robin Walters, Heiko Zimmermann, Jan-Willem van de Meent</em><br>

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://openreview.net/forum?id=1tviRBNxI9" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">Conference paper</a> · 
        <a href="https://openreview.net/pdf?id=1tviRBNxI9" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">PDF</a> · 
        <!-- GitHub link not provided -->
      </span>

      <strong>Abstract:</strong> Incorporating geometric inductive biases into models can aid interpretability and generalization, but encoding to a specific geometric structure can be challenging due to the imposed topological constraints. In this paper, we theoretically and empirically characterize obstructions to training encoders with geometric latent spaces. We show that local optima can arise due to singularities (e.g. self-intersection) or due to an incorrect degree or winding number. We then discuss how normalizing flows can potentially circumvent these obstructions by defining multimodal variational distributions. Inspired by this observation, we propose a new flow-based model that maps data points to multimodal distributions over geometric spaces and empirically evaluate our model on 2 domains. We observe improved stability during training and a higher chance of converging to a homeomorphic encoder. <br>

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #GeometricInductiveBiases #DataTopology #NormalizingFlows
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/top_obst.jpeg" alt="Graphical Abstract" style="height: auto; max-height: 90%; width: auto; max-width: 90%;  margin-left: 10%;">
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;">

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning</strong><br>
      <em>Fan Feng, Sara Magliacane</em><br>

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://arxiv.org/abs/2307.09205" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">Paper</a> · 
        <a href="https://arxiv.org/pdf/2307.09205.pdf" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">PDF</a> · 
        <!-- GitHub repo to be updated later -->
      </span>

      <strong>Abstract:</strong> In many reinforcement learning tasks, the agent has to learn to interact with many objects of different types and generalize to unseen combinations and numbers of objects. Often a task is a composition of previously learned tasks (e.g. block stacking). These are examples of compositional generalization, in which we compose object-centric representations to solve complex tasks. Recent works have shown the benefits of object-factored representations and hierarchical abstractions for improving sample efficiency in these settings. In this paper, we introduce the Dynamic Attribute FacTored RL (DAFT-RL) framework. In DAFT-RL, we leverage object-centric representation learning to extract objects from visual inputs, classify them in classes, and infer their latent parameters. For each class of object, we learn a class template graph that describes the dynamics and reward of an object of this class factorized according to its attributes. We also learn an interaction pattern graph that describes how objects of different classes interact with each other at the attribute level. Through these graphs and a dynamic interaction graph that models the interactions between objects, we learn a policy that can then be directly applied in a new environment by just estimating the interactions and latent parameters. We evaluate DAFT-RL in three benchmark datasets and show our framework outperforms the state-of-the-art in generalizing across unseen objects with varying attributes and latent parameters, as well as in the composition of previously learned tasks. <br>

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #ReinforcementLearning #ObjectCentricRepresentations
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/dyn_attr.png" alt="Graphical Abstract" style="height: auto; max-height: 100%; width: auto; max-width: 100%;  margin-left: 10%;">
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;">

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity</strong><br>
      <em>Metod Jazbec, James Urquhart Allingham, Dan Zhang, Eric Nalisnick</em><br>

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://arxiv.org/abs/2306.02652" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">Paper</a> · 
        <a href="https://github.com/metodj/AnytimeClassification" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">GitHub repo</a>
      </span>

      <strong>Abstract:</strong> Modern predictive models are often deployed to environments in which computational budgets are dynamic. Anytime algorithms are well-suited to such environments as, at any point during computation, they can output a prediction whose quality is a function of computation time. Early-exit neural networks have garnered attention in the context of anytime computation due to their capability to provide intermediate predictions at various stages throughout the network. However, we demonstrate that current early-exit networks are not directly applicable to anytime settings, as the quality of predictions for individual data points is not guaranteed to improve with longer computation. To address this shortcoming, we propose an elegant post-hoc modification, based on the Product-of-Experts, that encourages an early-exit network to become gradually confident. This gives our deep models the property of conditional monotonicity in the prediction quality -- an essential stepping stone towards truly anytime predictive modeling using early-exit architectures. Our empirical results on standard image-classification tasks demonstrate that such behaviors can be achieved while preserving competitive accuracy on average. <br>

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #AnytimeClassification #EarlyExitNetworks #ImageClassification
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/PA_twitter_v2.png" alt="Graphical Abstract" style="height: auto; max-height: 80%; width: auto; max-width: 80%;  margin-left: 10%;">
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;">

<div style="background-color: rgba(123, 104, 238, 0.1); padding: 10px 0; text-align: center;">
    <h2 style="color: #524364; font-weight: bold;">Workshops</h2>
    <img src="https://neurips.cc/media/Press/NeurIPS_logo.svg" alt="Workshops" width="180" height="90" style="margin: auto;">
</div>

<hr style="margin-top: 20px;">

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Switching policies for solving inverse problems</strong><br>
      <em>Tim Bakker, Fabio Valerio Massoli, Thomas Hehn, Tribhuvanesh Orekondy, Arash Behboodi</em><br>

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://openreview.net/forum?id=QaIEU0wWj8" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">Conference paper</a> · 
        <a href="https://openreview.net/pdf?id=QaIEU0wWj8" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">PDF</a> · 
        <!-- GitHub repo not provided -->
      </span>

      <strong>Abstract:</strong> In recent years, inverse problems for black-box simulators have enjoyed increased focus of the machine learning community due to their prevalence in science and engineering domains. Such simulators describe a forward process \(f: (\psi, x) \rightarrow y\). Here the intent is to optimise simulator parameters \(\psi\) to minimise some observation loss on \(y\), under some input distribution on \(x\). Optimisation of such objectives is often challenging, since it is not trivial to estimate simulator gradients accurately. In settings where multiple related inverse problems need to be solved simultaneously, from-scratch/ab-initio optimisation of each may be infeasible if the forward model is expensive to evaluate. In this paper, we propose a novel method for solving such families of inverse problems with reinforcement learning. We train a policy to guide the optimisation by selecting between gradients estimated numerically from the simulator and gradients estimated from a pre-trained surrogate model. After training the surrogate and the policy, downstream inverse problem optimisations require 10%-70% fewer simulator evaluations. Moreover, the policy does successful optimisations on functions where using just simulator gradient estimates fails.
 <br>

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #InverseProblems #ReinforcementLearning
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/tim2.png" alt="Graphical Abstract" style="height: auto; max-height: 80%; width: auto; max-width: 80%;  margin-left: 10%;">
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;">

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Active Learning Policies for Solving Inverse Problems</strong><br>
      <em>Tim Bakker, Thomas Hehn, Tribhuvanesh Orekondy, Arash Behboodi, Fabio Valerio Massoli</em><br>

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://openreview.net/forum?id=eLIk3m5C79" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">Conference paper</a>
        <!-- PDF and GitHub links not provided -->
      </span>

      <strong>Abstract:</strong> In recent years, solving inverse problems for black-box simulators has become a point of focus for the machine learning community due to their ubiquity in science and engineering scenarios. In such settings, the simulator describes a forward process \(f: (\psi, x) \rightarrow y\) from simulator parameters \(\psi\) and input data \(x\) to observations \(y\), and the goal of the inverse problem is to optimise \(\psi\) to minimise some observation loss. Simulator gradients are often unavailable or prohibitively expensive to obtain, making optimisation of these simulators particularly challenging. Moreover, in many applications, the goal is to solve a family of related inverse problems. Thus, starting optimisation ab-initio/from-scratch may be infeasible if the forward model is expensive to evaluate. In this paper, we propose a novel method for solving classes of similar inverse problems. We learn an active learning policy that guides the training of a surrogate and use the gradients of this surrogate to optimise the simulator parameters with gradient descent. After training the policy, downstream inverse problem optimisations require up to 90\% fewer forward model evaluations than the baseline. <br>

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #InverseProblems #ActiveLearning
      </span>

    </td>
    <td style="width: 50%;">
      <!-- Image placeholder since no image link is provided -->
      <img src="/assets/neurips2023/active.png" alt="Graphical Abstract" style="height: auto; max-height: 80%; width: auto; max-width: 80%;  margin-left: 10%;">
    </td>
  </tr>
</table>

<hr style="margin-top: 20px;">

<table>
  <tr>
    <td style="width: 50%;">
      <strong style="font-size: 20px;">Designing Long-term Group Fair Policies in Dynamical Systems</strong><br>
      <em>Miriam Rateike, Isabel Valera, Patrick Forré</em><br>

      <span style="font-size: 14px; display: block; margin-bottom: 10px;">
        <a href="https://arxiv.org/abs/2311.12447" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">Paper</a> · 
        <a href="https://arxiv.org/pdf/2311.12447.pdf" style="text-decoration: none; color: #007bff;" target="_blank" rel="noopener noreferrer">PDF</a> · 
        <!-- GitHub repo not provided -->
      </span>

      <strong>Abstract:</strong> This paper proposes a novel framework for achieving long-term group fairness in dynamical systems. The framework aims to identify a time-independent policy that ensures convergence to a fair stationary state, regardless of the initial data distribution. By modeling system dynamics with a time-homogeneous Markov chain and leveraging the Markov chain convergence theorem, the paper provides examples of various long-term fair states and demonstrates how the approach can evaluate the impact of different long-term targets on group-conditional population distribution over time. <br>

      <span style="font-size: 14px; font-weight: bold; color: #6c757d;">
        #LongTermFairness #DynamicalSystems #MarkovChains
      </span>

      </td>
      <td style="position: relative; width: 50%;">
        <img src="/assets/neurips2023/Long-term-fair-picture-abstract.png" alt="Graphical Abstract" style="height: auto; max-height: 70%; width: auto; max-width: 70%;  margin-left: 15%;">
        <span style="position: absolute; top: 14%; right: 0; background-color: #ffcc00; color: black; padding: 5px; font-size: 20px; transform: rotate(35deg); transform-origin: top right;">
          Oral Presentation
        </span>
      </td>
  </tr>
</table>

<hr style="margin-top: 20px;">


  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 AMLab  | Amsterdam Machine Learning Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

